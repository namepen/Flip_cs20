{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST convolutional neural networks with batch norm and regularizer\n",
    "\n",
    "* MNIST data를 가지고 softmax classifier를 만들어보자.\n",
    "* train data, validation data를 구분하여 model을 training하자\n",
    "  + train data : model을 training 시키는 데 사용\n",
    "  + validation data : training에 이용하지않은 data, model을 validation 하는 데 활용 (보통 1 epoch마다)\n",
    "* batch normalization과 l2 regularizer를 추가!\n",
    "* [`tf.contrib.slim`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) 참고\n",
    "* [05_mnist_cnn_with_train_validation_split.ipynb](https://nbviewer.jupyter.org/github/modulabs/modu-tensorflow/blob/master/week03/05_mnist_cnn_with_train_validation_split.ipynb)를 기반으로 refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A LeNet-5 like cnn MNIST classifier.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "#np.random.seed(219)\n",
    "#tf.set_random_seed(219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data from tf.keras\n",
    "(x_train, y_train), (x_test, y_test) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_train = x_train.astype(dtype = np.float32)\n",
    "y_train = np.asarray(y_train, dtype=np.int32)\n",
    "\n",
    "x_test = x_test / 255.\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_test = np.asarray(y_test, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train에서 training에 쓸 index 50000개 가져옴\n",
    "tr_indices = np.random.choice(np.arange(x_train.shape[0]), size = 50000, replace = False)\n",
    "\n",
    "# model training에 이용할 data\n",
    "x_tr = x_train[tr_indices]\n",
    "y_tr = y_train[tr_indices]\n",
    "\n",
    "# epoch 마다 model validation에 이용할 data\n",
    "x_val = np.delete(arr = x_train, obj = tr_indices, axis = 0)\n",
    "y_val = np.delete(arr = y_train, obj = tr_indices, axis = 0)\n",
    "\n",
    "print('for training {}, {}'.format(x_tr.shape, y_tr.shape))\n",
    "print('for validation {}, {}'.format(x_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dataset with `tf.data`\n",
    "\n",
    "#### create input pipeline with `tf.data.Dataset` to train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# for training\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((x_tr, y_tr))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size = 10000)\n",
    "tr_dataset = tr_dataset.batch(batch_size = batch_size)\n",
    "tr_iterator = tr_dataset.make_initializable_iterator()\n",
    "\n",
    "print(tr_dataset)\n",
    "\n",
    "# for validation\n",
    "# validation data의 용량이 in memory에 넣을 수 없을정도로 아래와 같이 활용한다.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size = 10000)\n",
    "val_dataset = val_dataset.batch(batch_size = batch_size)\n",
    "val_iterator = val_dataset.make_initializable_iterator()\n",
    "\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Iterator.from_string_handle의 output_shapes는 default = None이지만 꼭 값을 넣는 게 좋음\n",
    "handle = tf.placeholder(tf.string)\n",
    "iterator = tf.data.Iterator.from_string_handle(handle,\n",
    "                                               tr_dataset.output_types,\n",
    "                                               tr_dataset.output_shapes)\n",
    "x, y = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(x):\n",
    "    \"\"\"\"Model function for CNN.\n",
    "    Args:\n",
    "        x: input images\n",
    "        mode: boolean whether trainig mode or test mode\n",
    "    Returns:\n",
    "    logits: unnormalized score funtion\n",
    "  \"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # MNIST images are 28x28 pixels, and have one color channel\n",
    "\n",
    "    # batch normalization 적용 여부를 control하기위한 placeholder \n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "    # slim.arg_scope를 이용하여, slim.conv2d, slim.fully_connected 함수의 default로 batch norm과\n",
    "    # l2 regularizer를 설정 (scale = 0.0003)\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                         weights_regularizer = slim.l2_regularizer(scale = .0003),\n",
    "                         normalizer_fn = slim.batch_norm,\n",
    "                         normalizer_params = {'decay' : .9, 'is_training': is_training}):\n",
    "        # Convolutional Layer #1\n",
    "        # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "        # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "        conv1 = slim.conv2d(x_image, 32, [5, 5], scope='conv1')\n",
    "\n",
    "        # Pooling Layer #1\n",
    "        # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "        # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "        pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\n",
    "  \n",
    "        # Convolutional Layer #2\n",
    "        # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "        # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "        conv2 = slim.conv2d(pool1, 64, [5, 5], scope='conv2')\n",
    "\n",
    "        # Pooling Layer #2\n",
    "        # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "        # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "        # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "        pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\n",
    "\n",
    "        # Flatten tensor into a batch of vectors\n",
    "        # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "        # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "        pool2_flat = slim.flatten(pool2, scope='flatten')\n",
    "  \n",
    "        # Fully connected Layer\n",
    "        # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "        # Output Tensor Shape: [batch_size, 1024]\n",
    "        fc1 = slim.fully_connected(pool2_flat, 1024, scope='fc1')\n",
    "\n",
    "        # Logits layer\n",
    "        # Input Tensor Shape: [batch_size, 1024]\n",
    "        # Output Tensor Shape: [batch_size, 10]\n",
    "        logits = slim.fully_connected(fc1, 10, activation_fn=None, normalizer_fn = None,\n",
    "                                      normalizer_params = None,\n",
    "                                      scope='logits')\n",
    "  \n",
    "    return logits, is_training, x_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, is_training, x_image = cnn_model_fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "# tf.get_collection과 tf.GraphKeys.REGULARIZATION_LOSSES를 이용하여, regularization loss를 가져옴\n",
    "reg_term = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "total_loss = cross_entropy + reg_term\n",
    "\n",
    "# loss를 계산하기전 training의 경우, mini-batch 당 mean과 variance를 계산해야하고, \n",
    "# validation의 경우 training시 mini-batch당 가지고 있던 mean과 variance를 exponential moving average를 이용하여\n",
    "# average한 것을 가지고 있다가 이용해야함. 아래의 코드가 그 기능을 함\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(control_inputs = update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session()` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# history\n",
    "tr_hist = []\n",
    "val_hist = []\n",
    "\n",
    "# Generate handles of tr_iterator and val_iterator\n",
    "tr_handle, val_handle = sess.run(fetches = [tr_iterator.string_handle(), val_iterator.string_handle()])\n",
    "\n",
    "# Train\n",
    "max_epochs = 5\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    avg_tr_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    tr_step = 0\n",
    "    val_step = 0\n",
    "    \n",
    "    # training 1-epoch\n",
    "    sess.run(tr_iterator.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            _, tr_loss = sess.run(fetches = [train_step, total_loss],\n",
    "                               feed_dict = {handle : tr_handle, is_training : True})\n",
    "            tr_step += 1\n",
    "            avg_tr_loss += tr_loss\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    \n",
    "    # validation 1-epoch\n",
    "    sess.run(val_iterator.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            val_loss = sess.run(total_loss, \n",
    "                                feed_dict = {handle : val_handle, is_training : False})\n",
    "            val_step += 1\n",
    "            avg_val_loss += val_loss\n",
    "        \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            \n",
    "    avg_tr_loss /= tr_step\n",
    "    avg_val_loss /= val_step\n",
    "    tr_hist.append(avg_tr_loss)\n",
    "    val_hist.append(avg_val_loss)\n",
    "    \n",
    "    print('epochs : {:3}, tr_loss : {:.3f}, val_loss : {:.3f}'.format(epoch + 1, avg_tr_loss, avg_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = sess.run(logits, feed_dict = {x : x_test, is_training : False})\n",
    "yhat = np.argmax(yhat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test accuracy : {:.2%}'.format(np.mean(yhat == y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_hist, label = 'train')\n",
    "plt.plot(val_hist, label = 'validation')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
