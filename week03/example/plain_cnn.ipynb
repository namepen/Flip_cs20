{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST convolutional neural networks with slim\n",
    "\n",
    "* MNIST data를 가지고 softmax classifier를 만들어보자.\n",
    "* [`tf.contrib.slim`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A LeNet-5 like cnn MNIST classifier.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "np.random.seed(219)\n",
    "tf.set_random_seed(219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = 500\n",
    "print(\"label = {}\".format(train_labels[index]))\n",
    "plt.imshow(train_data[index].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dataset with `tf.data`\n",
    "\n",
    "#### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>\n",
      "<BatchDataset shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "test_dataset = test_dataset.shuffle(buffer_size = 10000)\n",
    "test_dataset = test_dataset.batch(batch_size = len(test_data))\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Iterator.from_string_handle의 output_shapes는 default = None이지만 꼭 값을 넣는 게 좋음\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle,\n",
    "                                               train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "x, y = iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.contrib.slim`\n",
    "\n",
    "```python\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# pylint: disable=unused-import,line-too-long,g-importing-member,wildcard-import\n",
    "# TODO(jart): Delete non-slim imports\n",
    "from tensorflow.contrib import losses\n",
    "from tensorflow.contrib import metrics\n",
    "from tensorflow.contrib.framework.python.ops.arg_scope import *\n",
    "from tensorflow.contrib.framework.python.ops.variables import *\n",
    "from tensorflow.contrib.layers.python.layers import *\n",
    "from tensorflow.contrib.layers.python.layers.initializers import *\n",
    "from tensorflow.contrib.layers.python.layers.regularizers import *\n",
    "from tensorflow.contrib.slim.python.slim import evaluation\n",
    "from tensorflow.contrib.slim.python.slim import learning\n",
    "from tensorflow.contrib.slim.python.slim import model_analyzer\n",
    "from tensorflow.contrib.slim.python.slim import queues\n",
    "from tensorflow.contrib.slim.python.slim import summaries\n",
    "from tensorflow.contrib.slim.python.slim.data import data_decoder\n",
    "from tensorflow.contrib.slim.python.slim.data import data_provider\n",
    "from tensorflow.contrib.slim.python.slim.data import dataset\n",
    "from tensorflow.contrib.slim.python.slim.data import dataset_data_provider\n",
    "from tensorflow.contrib.slim.python.slim.data import parallel_reader\n",
    "from tensorflow.contrib.slim.python.slim.data import prefetch_queue\n",
    "from tensorflow.contrib.slim.python.slim.data import tfexample_decoder\n",
    "from tensorflow.python.util.all_util import make_all\n",
    "# pylint: enable=unused-import,line-too-long,g-importing-member,wildcard-import\n",
    "\n",
    "__all__ = make_all(__name__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers) and [`tf.contrib.layers`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers)\n",
    "\n",
    "#### [`tf.layers.conv2d()`](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d)\n",
    "```python\n",
    "tf.layers.conv2d(\n",
    "    inputs,\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=(1, 1),\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=None,\n",
    "    bias_initializer=tf.zeros_initializer(),\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    trainable=True,\n",
    "    name=None,\n",
    "    reuse=None\n",
    ")\n",
    "```\n",
    "\n",
    "#### [`slim.conv2d()`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d)\n",
    "```python\n",
    "tf.contrib.layers.conv2d(\n",
    "    inputs,\n",
    "    num_outputs,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding='SAME',\n",
    "    data_format=None,\n",
    "    rate=1,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=initializers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(x):\n",
    "    \n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "        \n",
    "        conv = slim.conv2d(x_image, 16, [3,3], scope='init_conv')\n",
    "        #output : [batch_size, 28, 28, 16]\n",
    "        \n",
    "        with tf.variable_scope(\"first_box\", num_outputs = 16, kernel_size = [3,3]):\n",
    "            conv1 = slim.conv2d(conv, \"conv1\")\n",
    "            conv2 = slim.conv2d(con1, \"conv2\")\n",
    "            conv3 = slim.conv2d(con2, \"conv3\")\n",
    "            conv4 = slim.conv2d(con3, \"conv4\")\n",
    "            \n",
    "        pool1 = slim.conv2d(conv4, kernel_size=[2,2], scope='pool1')\n",
    "        #output : [batch_size, 14,14, 16]\n",
    "        \n",
    "        with tf.variable_scope(\"second_box\", num_outputs = 32, kernel_size = [3,3]):\n",
    "            conv1 = slim.conv2d(conv4, \"conv1\")\n",
    "            conv2 = slim.conv2d(con1, \"conv2\")\n",
    "            conv3 = slim.conv2d(con2, \"conv3\")\n",
    "            conv4 = slim.conv2d(con3, \"conv4\")\n",
    "            \n",
    "        pool2 = slim.conv2d(conv4, kernel_size=[2,2], scope='pool1')\n",
    "        #output : [batch_size, 7,7, 16]\n",
    "        \n",
    "        with tf.variable_scope(\"third_box\", num_outputs = 64, kernel_size = [3,3]):\n",
    "            conv1 = slim.conv2d(conv4, \"conv1\")\n",
    "            conv2 = slim.conv2d(con1, \"conv2\")\n",
    "            conv3 = slim.conv2d(con2, \"conv3\")\n",
    "            conv4 = slim.conv2d(con3, \"conv4\")\n",
    "            \n",
    "        fc1 = slim.fully_connected(conv4, 1024, scope='fc1')\n",
    "        \n",
    "        is_training = tf.placeholder(tf.bool, shape=[])\n",
    "        fc_drop = slim.dropout(fc1, keep_prob=0.7, scope='dropout')\n",
    "        \n",
    "        logits = slim.fully_connected(fc1_drop, 10, activation_fn=None, scope='logits')\n",
    "        \n",
    "        return logits, is_training, x_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-144bd1e22fc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-1391e92d2d90>\u001b[0m in \u001b[0;36mcnn_model_fn\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#output : [batch_size, 28, 28, 16]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first_box\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mconv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"conv1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mconv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcon1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"conv2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_outputs'"
     ]
    }
   ],
   "source": [
    "logits, is_training, x_image = cnn_model_fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a8ad5d0ed76e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_one_hot, logits=logits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m cross_entropy = tf.losses.softmax_cross_entropy(labels=y,\n\u001b[1;32m----> 5\u001b[1;33m                                                        logits=logits)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "#y_one_hot = tf.one_hot(y, depth=10)\n",
    "#cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_one_hot, logits=logits)\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y,\n",
    "                                                       logits=logits)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign `tf.summary.FileWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_location = 'graphs/02_mnist_cnn_with_slim'\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "  tf.summary.scalar('loss/cross_entropy', cross_entropy)\n",
    "  tf.summary.image('images', x_image)\n",
    "  for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "  # merge all summaries\n",
    "  summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session()` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train_iterator\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "train_handle = sess.run(train_iterator.string_handle())\n",
    "\n",
    "# Train\n",
    "max_epochs = 2\n",
    "step = 0\n",
    "for epochs in range(max_epochs):\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  sess.run(train_iterator.initializer)\n",
    "\n",
    "  start_time = time.time()\n",
    "  while True:\n",
    "    try:\n",
    "      # 여기를 직접 채워 넣으시면 됩니다.\n",
    "      _, loss = sess.run([train_step, cross_entropy],\n",
    "                         feed_dict={handle: train_handle,\n",
    "                                    is_training: True})\n",
    "      if step % 100 == 0:\n",
    "        print(\"step: {}, loss: {}\".format(step, loss))\n",
    "        \n",
    "        # summary\n",
    "        summary_str = sess.run(summary_op, feed_dict={handle: train_handle, is_training: False})\n",
    "        train_writer.add_summary(summary_str, global_step=step)\n",
    "        \n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break\n",
    "    \n",
    "  print(\"Epochs: {} Elapsed time: {}\".format(epochs, time.time() - start_time))\n",
    "\n",
    "train_writer.close()\n",
    "print(\"training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_iterator\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "test_handle = sess.run(test_iterator.string_handle())\n",
    "sess.run(test_iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, acc_op = tf.metrics.accuracy(labels=y, predictions=tf.argmax(logits, 1), name='accuracy')\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "sess.run(acc_op, feed_dict={handle: test_handle, is_training: False})\n",
    "print(\"test accuracy:\", sess.run(accuracy, feed_dict={handle: test_handle, is_training: False}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_batch_size = 16\n",
    "batch_index = np.random.choice(len(test_data), size=test_batch_size, replace=False)\n",
    "batch_xs = test_data[batch_index]\n",
    "y_pred = sess.run(logits, feed_dict={x: batch_xs, is_training: False})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "for i, (px, py) in enumerate(zip(batch_xs, y_pred)):\n",
    "  p = fig.add_subplot(4, 8, i+1)\n",
    "  p.set_title(\"y_pred: {}\".format(np.argmax(py)))\n",
    "  p.imshow(px.reshape(28, 28), cmap='gray')\n",
    "  p.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
