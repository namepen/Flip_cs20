{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN with MNIST\n",
    "\n",
    "* MNIST DATA를 가지고 Deep Convoltuion Gan을 tf.slim을 이용해서 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#tf.set_random_seed(219)\n",
    "#np.random.seed(219)\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "train_dir = './DCGAN/train/'\n",
    "max_epochs = 30\n",
    "save_epochs = 10\n",
    "summary_steps = 2500\n",
    "print_steps = 1000\n",
    "batch_size = 64\n",
    "learning_rate_D = 0.0002\n",
    "learning_rate_G = 0.001\n",
    "k = 1 # the number of step of learning D before learning G\n",
    "num_samples = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/home/Downloads/agar_plate_ey/\n"
     ]
    }
   ],
   "source": [
    "path_dir = '/Users/home/Downloads/agar_plate_ey/'\n",
    "file_list = os.listdir(path_dir)\n",
    "\n",
    "print(path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/home/Downloads/agar_plate_ey/img_106.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir+file_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in enumerate(file_list):\n",
    "    img = cv2.imread(path_dir + i[1], 0)\n",
    "    #img = np.array(img)\n",
    "    #print(img.shape)\n",
    "    img = img / 255\n",
    "    train_data.append(img)\n",
    "    \n",
    "train_data = np.asanyarray(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dataset with `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (?, 28, 28), types: tf.float64>\n"
     ]
    }
   ],
   "source": [
    "#for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count= max_epochs)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "  \"\"\"Deep Convolutional Generative Adversarial Networks\n",
    "  implementation based on http://arxiv.org/abs/1511.06434\n",
    "  \n",
    "  \"Unsupervised Representation Learning with\n",
    "  Deep Convolutional Generative Adversarial Networks\"\n",
    "  Alec Radford, Luke Metz and Soumith Chintala\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, mode, train_dataset, test_dataset=None):\n",
    "    \"\"\"Basic setup.\n",
    "    \n",
    "    Args:\n",
    "      mode (`string`): \"train\" or \"generate\".\n",
    "      train_dataset (`tf.data.Dataset`): train_dataset.\n",
    "      test_dataset (`tf.data.Dataset`): test_dataset.\n",
    "    \"\"\"\n",
    "    assert mode in [\"train\", \"generate\"]\n",
    "    self.mode = mode\n",
    "    \n",
    "    # hyper-parameters for model\n",
    "    self.x_dim = 28\n",
    "    self.z_dim = 100\n",
    "    self.batch_size = batch_size\n",
    "    self.num_samples = num_samples\n",
    "    self.train_dataset = train_dataset\n",
    "    self.test_dataset = test_dataset\n",
    "    \n",
    "    # Global step Tensor.\n",
    "    self.global_step = None\n",
    "    \n",
    "    print('The mode is %s.' % self.mode)\n",
    "    print('complete initializing model.')\n",
    "    \n",
    "    \n",
    "  def build_random_z_inputs(self):\n",
    "    \"\"\"Build a vector random_z in latent space.\n",
    "    \n",
    "    Returns:\n",
    "      self.random_z (`4-rank Tensor` with [batch_size, 1, 1, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "      self.sample_random_z (`4-rank Tensor` with [num_samples, 1, 1, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "    \"\"\"\n",
    "    # 여기를 채워 넣으세요\n",
    "    # Setup variable of random vector z\n",
    "    with tf.variable_scope('random_z'):\n",
    "      self.random_z = tf.random_uniform(shape=[self.batch_size, 1, 1, self.z_dim],\n",
    "                                        minval=-1.0, maxval=1.0)\n",
    "      self.sample_random_z = tf.random_uniform(shape=[self.num_samples, 1, 1, self.z_dim],\n",
    "                                               minval=-1.0, maxval=1.0)\n",
    "\n",
    "    return self.random_z, self.sample_random_z\n",
    "  \n",
    "  \n",
    "  def read_MNIST(self, dataset):\n",
    "    \"\"\"Read MNIST dataset\n",
    "    \n",
    "    Args:\n",
    "      dataset (`tf.data.Dataset` format): MNIST dataset.\n",
    "      \n",
    "    Returns:\n",
    "      self.mnist (`4-rank Tensor` with [batch, x_dim, x_dim, 1]): MNIST dataset with batch size.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('mnist'):\n",
    "      iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "      self.mnist = iterator.get_next()\n",
    "      self.mnist = tf.cast(self.mnist, dtype = tf.float32)\n",
    "      self.mnist = tf.expand_dims(self.mnist, axis=3)\n",
    "      \n",
    "    return self.mnist\n",
    "\n",
    "\n",
    "  def Generator(self, random_z, is_training=True, reuse=False):\n",
    "    \"\"\"Generator setup.\n",
    "    \n",
    "    Args:\n",
    "      random_z (`2-rank Tensor` with [batch_size, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "      is_training (`bool`): whether training mode or test mode.\n",
    "      reuse (`bool`): whether variable reuse or not.\n",
    "      \n",
    "    Returns:\n",
    "      generated_data (`4-rank Tensor` with [batch_size, h, w, c])\n",
    "          generated images from random vector z.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Generator', reuse=reuse) as scope:\n",
    "      batch_norm_params = {'decay': 0.9,\n",
    "                           'epsilon': 0.001,\n",
    "                           'is_training': is_training,\n",
    "                           'scope': 'batch_norm'}\n",
    "      with slim.arg_scope([slim.conv2d_transpose],\n",
    "                          kernel_size=[4, 4],\n",
    "                          stride=[2, 2],\n",
    "                          normalizer_fn=slim.batch_norm,\n",
    "                          normalizer_params=batch_norm_params):\n",
    "        # 여기를 채워 넣으세요\n",
    "        # Use full conv2d_transpose instead of projection and reshape\n",
    "        # random_z: 1 x 1 x 100 dim\n",
    "        self.inputs = random_z\n",
    "        # inputs = random_z: 1 x 1 x 100 dim\n",
    "        # outputs: 3 x 3 x 256 dim\n",
    "        self.layer1 = slim.conv2d_transpose(inputs=self.inputs,\n",
    "                                            num_outputs=256,\n",
    "                                            kernel_size=[3, 3],\n",
    "                                            padding='VALID',\n",
    "                                            scope='layer1')\n",
    "        # inputs: 3 x 3 x 256 dim\n",
    "        # outputs: 7 x 7 x 128 dim\n",
    "        self.layer2 = slim.conv2d_transpose(inputs=self.layer1,\n",
    "                                            num_outputs=128,\n",
    "                                            kernel_size=[3, 3],\n",
    "                                            padding='VALID',\n",
    "                                            scope='layer2')\n",
    "        # inputs: 7 x 7 x 128 dim\n",
    "        # outputs: 14 x 14 x 64 dim\n",
    "        self.layer3 = slim.conv2d_transpose(inputs=self.layer2,\n",
    "                                            num_outputs=64,\n",
    "                                            scope='layer3')\n",
    "        # inputs: 14 x 14 x 64 dim\n",
    "        # outputs: 28 x 28 x 1 dim\n",
    "        self.layer4 = slim.conv2d_transpose(inputs=self.layer3,\n",
    "                                            num_outputs=1,\n",
    "                                            normalizer_fn=None,\n",
    "                                            activation_fn=tf.sigmoid,\n",
    "                                            scope='layer4')\n",
    "        # generated_data = outputs: 28 x 28 x 1 dim\n",
    "        generated_data = self.layer4\n",
    "\n",
    "        return generated_data\n",
    "    \n",
    "    \n",
    "  def Discriminator(self, data, reuse=False):\n",
    "    \"\"\"Discriminator setup.\n",
    "    \n",
    "    Args:\n",
    "      data (`2-rank Tensor` with [batch_size, x_dim]): MNIST real data.\n",
    "      reuse (`bool`): whether variable reuse or not.\n",
    "      \n",
    "    Returns:\n",
    "      logits (`1-rank Tensor` with [batch_size]): logits of data.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator', reuse=reuse) as scope:\n",
    "      batch_norm_params = {'decay': 0.9,\n",
    "                           'epsilon': 0.001,\n",
    "                           'scope': 'batch_norm'}\n",
    "      with slim.arg_scope([slim.conv2d],\n",
    "                          kernel_size=[4, 4],\n",
    "                          stride=[2, 2],\n",
    "                          activation_fn=tf.nn.leaky_relu,\n",
    "                          normalizer_fn=slim.batch_norm,\n",
    "                          normalizer_params=batch_norm_params):\n",
    "        # 여기를 채워 넣으세요\n",
    "        # inputs = data: 28 x 28 x 1 dim\n",
    "        # outputs: 14 x 14 x 64 dim\n",
    "        self.layer1 = slim.conv2d(inputs=data,\n",
    "                                  num_outputs=64,\n",
    "                                  normalizer_fn=None,\n",
    "                                  scope='layer1')\n",
    "        # inputs: 14 x 14 x 64 dim\n",
    "        # outputs: 7 x 7 x 128 dim\n",
    "        self.layer2 = slim.conv2d(inputs=self.layer1,\n",
    "                                  num_outputs=128,\n",
    "                                  scope='layer2')\n",
    "        # inputs: 7 x 7 x 128 dim\n",
    "        # outputs: 3 x 3 x 256 dim\n",
    "        self.layer3 = slim.conv2d(inputs=self.layer2,\n",
    "                                  num_outputs=256,\n",
    "                                  kernel_size=[3, 3],\n",
    "                                  padding='VALID',\n",
    "                                  scope='layer3')\n",
    "        # inputs: 3 x 3 x 256 dim\n",
    "        # outputs: 1 x 1 x 1 dim\n",
    "        self.layer4 = slim.conv2d(inputs=self.layer3,\n",
    "                                  num_outputs=1,\n",
    "                                  kernel_size=[3, 3],\n",
    "                                  stride=[1, 1],\n",
    "                                  padding='VALID',\n",
    "                                  normalizer_fn=None,\n",
    "                                  activation_fn=None,\n",
    "                                  scope='layer4')\n",
    "        # logits = layer4: 1 x 1 x 1 dim -> 1 dim\n",
    "        discriminator_logits = tf.squeeze(self.layer4, axis=[1, 2])\n",
    "\n",
    "        return discriminator_logits\n",
    "    \n",
    "    \n",
    "  def setup_global_step(self):\n",
    "    \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "    if self.mode == \"train\":\n",
    "      self.global_step = tf.train.get_or_create_global_step()\n",
    "      \n",
    "      print('complete setup global_step.')\n",
    "      \n",
    "      \n",
    "  def GANLoss(self, logits, is_real=True, scope=None):\n",
    "    \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "    \n",
    "    Args:\n",
    "      logits (`1-rank Tensor`): logits.\n",
    "      is_real (`bool`): True means `1` labeling, False means `0` labeling.\n",
    "      \n",
    "    Returns:\n",
    "      loss (`0-randk Tensor): the standard GAN loss value. (binary_cross_entropy)\n",
    "    \"\"\"\n",
    "    if is_real:\n",
    "      labels = tf.ones_like(logits)\n",
    "    else:\n",
    "      labels = tf.zeros_like(logits)\n",
    "\n",
    "    # 여기를 채워 넣으세요\n",
    "    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels,\n",
    "                                           logits=logits,\n",
    "                                           scope=scope)\n",
    "\n",
    "    return loss\n",
    "\n",
    "      \n",
    "  def build(self):\n",
    "    \"\"\"Creates all ops for training or generate.\"\"\"\n",
    "    self.setup_global_step()\n",
    "    \n",
    "    if self.mode == \"generate\":\n",
    "      pass\n",
    "    \n",
    "    else:\n",
    "      # generating random vector\n",
    "      self.random_z, self.sample_random_z = self.build_random_z_inputs()\n",
    "      # read dataset\n",
    "      self.real_data = self.read_MNIST(self.train_dataset)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # generating images from Generator() via random vector z\n",
    "      self.generated_data = self.Generator(self.random_z)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # discriminating real data by Discriminator()\n",
    "      self.real_logits = self.Discriminator(self.real_data)\n",
    "      # discriminating fake data (generated)_images) by Discriminator()\n",
    "      self.fake_logits = self.Discriminator(self.generated_data, reuse=True)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # losses of real with label \"1\"\n",
    "      self.loss_real = self.GANLoss(self.real_logits, is_real=True, scope='loss_D_real')\n",
    "      # losses of fake with label \"0\"\n",
    "      self.loss_fake = self.GANLoss(self.fake_logits, is_real=False, scope='loss_D_fake')\n",
    "      \n",
    "      # losses of Discriminator\n",
    "      with tf.variable_scope('loss_D'):\n",
    "        self.loss_Discriminator = self.loss_real + self.loss_fake\n",
    "        \n",
    "      # 여기를 채워 넣으세요\n",
    "      # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "      self.loss_Generator = self.GANLoss(self.fake_logits, is_real=True, scope='loss_G')\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # Separate variables for each function\n",
    "      self.D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n",
    "      self.G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n",
    "      \n",
    "      \n",
    "      # generating images for sample\n",
    "      self.sample_data = self.Generator(self.sample_random_z, is_training=False, reuse=True)\n",
    "      \n",
    "      # write summaries\n",
    "      # Add loss summaries\n",
    "      tf.summary.scalar('losses/loss_Discriminator', self.loss_Discriminator)\n",
    "      tf.summary.scalar('losses/loss_Generator', self.loss_Generator)\n",
    "      \n",
    "      # Add histogram summaries\n",
    "      for var in self.D_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      for var in self.G_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      \n",
    "      # Add image summaries\n",
    "      tf.summary.image('random_images', self.generated_data, max_outputs=4)\n",
    "      tf.summary.image('real_images', self.real_data)\n",
    "      \n",
    "    print('complete model build.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define plot fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_data(sample_data, max_print=num_samples):\n",
    "  print_images = sample_data[:max_print,:]\n",
    "  print_images = print_images.reshape([max_print, 28, 28])\n",
    "  print_images = print_images.swapaxes(0, 1)\n",
    "  print_images = print_images.reshape([28, max_print * 28])\n",
    "  \n",
    "  plt.figure(figsize=(max_print, 1))\n",
    "  plt.axis('off')\n",
    "  plt.imshow(print_images, cmap='gray')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulid the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mode is train.\n",
      "complete initializing model.\n",
      "complete setup global_step.\n",
      "complete model build.\n",
      "\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "Generator/layer1/weights:0 (float32_ref 3x3x256x100) [230400, bytes: 921600]\n",
      "Generator/layer1/batch_norm/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "Generator/layer2/weights:0 (float32_ref 3x3x128x256) [294912, bytes: 1179648]\n",
      "Generator/layer2/batch_norm/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "Generator/layer3/weights:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
      "Generator/layer3/batch_norm/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "Generator/layer4/weights:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]\n",
      "Generator/layer4/biases:0 (float32_ref 1) [1, bytes: 4]\n",
      "Discriminator/layer1/weights:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]\n",
      "Discriminator/layer1/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "Discriminator/layer2/weights:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
      "Discriminator/layer2/batch_norm/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "Discriminator/layer3/weights:0 (float32_ref 3x3x128x256) [294912, bytes: 1179648]\n",
      "Discriminator/layer3/batch_norm/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "Discriminator/layer4/weights:0 (float32_ref 3x3x256x1) [2304, bytes: 9216]\n",
      "Discriminator/layer4/biases:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 1087618\n",
      "Total bytes of variables: 4350472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1087618, 4350472)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DCGAN(mode=\"train\", train_dataset=train_dataset)\n",
    "model.build()\n",
    "\n",
    "# show info for trainable variables\n",
    "t_vars = tf.trainable_variables()\n",
    "slim.model_analyzer.analyze_vars(t_vars, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?slim.conv2d_transpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_global_step(self):\n",
    "    \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "    if self.mode == \"train\":\n",
    "      self.global_step = tf.train.get_or_create_global_step() #전체 training step을 관리하기 위한 함수\n",
    "      \n",
    "      print('complete setup global_step.')\n",
    "      \n",
    "      \n",
    "    def GANLoss(self, logits, is_real=True, scope=None):\n",
    "        \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "\n",
    "        Args:\n",
    "          logits (`1-rank Tensor`): logits.\n",
    "          is_real (`bool`): True means `1` labeling, False means `0` labeling.\n",
    "\n",
    "        Returns:\n",
    "          loss (`0-randk Tensor): the standard GAN loss value. (binary_cross_entropy)\n",
    "        \"\"\"\n",
    "        if is_real:\n",
    "          labels = tf.ones_like(logits)\n",
    "        else:\n",
    "          labels = tf.zeros_like(logits)\n",
    "\n",
    "        loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits, scope=scope)\n",
    "        return loss\n",
    "\n",
    "      \n",
    "    def build(self):\n",
    "        \"\"\"Creates all ops for training or generate.\"\"\"\n",
    "        self.setup_global_step()\n",
    "\n",
    "        if self.mode == \"generate\":\n",
    "          pass\n",
    "\n",
    "        else:\n",
    "          # generating random vector\n",
    "          self.random_z, self.sample_random_z = self.build_random_z_inputs()\n",
    "          # read dataset\n",
    "          self.real_data = self.read_MNIST(self.train_dataset)\n",
    "\n",
    "          # 여기를 채워 넣으세요\n",
    "          # generating images from Generator() via random vector z\n",
    "          self.generated_data = self.Generator(self.random_z, reuse = False)\n",
    "\n",
    "          # 여기를 채워 넣으세요\n",
    "          # discriminating real data by Discriminator()\n",
    "          self.real_logits = self.Discriminator(self.real_data, reuse = False)\n",
    "          # discriminating fake data (generated)_images) by Discriminator()\n",
    "          self.fake_logits = self.Discriminator(self.generated_data, reuse = True)\n",
    "\n",
    "          # 여기를 채워 넣으세요, def GANLoss(self, logits, is_real=True, scope=None)\n",
    "          # losses of real with label \"1\"\n",
    "          self.loss_real = self.GANLoss(self.real_logits, is_real= True, scope='loss_D_real')\n",
    "          # losses of fake with label \"0\"\n",
    "          self.loss_fake = self.GANLoss(self.fake_logits, is_real= False, scope='loss_D_fake')\n",
    "\n",
    "          # losses of Discriminator\n",
    "          with tf.variable_scope('loss_D'):\n",
    "            self.loss_Discriminator = self.loss_real + self.loss_fake\n",
    "\n",
    "          # 여기를 채워 넣으세요\n",
    "          # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "          self.loss_Generator = self.GANLoss(self.fake_logits, is_real = True, scope='loss_G')\n",
    "\n",
    "          # 여기를 채워 넣으세요\n",
    "          # Separate variables for each function\n",
    "          self.D_vars = tf.get_collection() #D에 관한 변수를 모을 collection\n",
    "          self.G_vars = tf.get_collection() #G에 관한 변수를 모을 collection\n",
    "\n",
    "\n",
    "          # generating images for sample\n",
    "          self.sample_data = self.Generator(self.sample_random_z, is_training=False, reuse=True)\n",
    "\n",
    "          # write summaries\n",
    "          # Add loss summaries\n",
    "          tf.summary.scalar('losses/loss_Discriminator', self.loss_Discriminator)\n",
    "          tf.summary.scalar('losses/loss_Generator', self.loss_Generator)\n",
    "\n",
    "          # Add histogram summaries\n",
    "          for var in self.D_vars:\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "          for var in self.G_vars:\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "          # Add image summaries\n",
    "          tf.summary.image('random_images', self.generated_data, max_outputs=4)\n",
    "          tf.summary.image('real_images', self.real_data)\n",
    "\n",
    "    print('complete model build.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_D = tf.train.AdamOptimizer(learning_rate=learning_rate_D, beta1=0.5)\n",
    "opt_G = tf.train.AdamOptimizer(learning_rate=learning_rate_G, beta1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Discriminator')):\n",
    "  opt_D_op = opt_D.minimize(model.loss_Discriminator, var_list=model.D_vars)\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Generator')):\n",
    "  opt_G_op = opt_G.minimize(model.loss_Generator, global_step=model.global_step,\n",
    "                            var_list=model.G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    \n",
    "    def __init__(self, mode, train_dataset, test_dataset = None):\n",
    "        \"\"\"Basic setup.\n",
    "\n",
    "        Args:\n",
    "          mode (`string`): \"train\" or \"generate\".\n",
    "          train_dataset (`tf.data.Dataset`): train_dataset.\n",
    "          test_dataset (`tf.data.Dataset`): test_dataset.\n",
    "        \"\"\"\n",
    "        assert mode in [\"train\", \"generate\"]\n",
    "        self.mode = mode\n",
    "        self.x_dim = 28 #mnist image dim\n",
    "        self.z_dim = 100 #Random Noise dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = num_samples\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        #Global step Tensor\n",
    "        self.global_step = None\n",
    "        \n",
    "        print('The mode is %s.' % self.mode)\n",
    "        print('complete initializing model.')\n",
    "        \n",
    "    def build_random_z_inputs(self, dataset):\n",
    "        \"\"\"Build a vector random_z in latent space.\n",
    "\n",
    "        Returns:\n",
    "          self.random_z (`2-rank Tensor` with [batch_size, z_dim]):\n",
    "              latent vector which size is generally 100 dim.\n",
    "          self.sample_random_z (`2-rank Tensor` with [num_samples, z_dim]):\n",
    "              latent vector which size is generally 100 dim.\n",
    "        \"\"\"\n",
    "        with tf.variable_op_scope('Random_z'):\n",
    "            self.random_z = tf.random_uniform(shape=[self.batch_size, z_dim], minval= -1.0, maxval= 1.0)\n",
    "            self.sample_random_z = tf.random_uniform(shape=[self.num_samples, z_dim], minval=-1.0, maxval= 1.0)\n",
    "            \n",
    "        return self.random_z, self.sample_random_z\n",
    "         \n",
    "        \n",
    "    def read_MNIST(self, dataset):\n",
    "        \"\"\"Read MNIST dataset\n",
    "\n",
    "        Args:\n",
    "          dataset (`tf.data.Dataset` format): MNIST dataset.\n",
    "\n",
    "        Returns:\n",
    "          self.mnist (`4-rank Tensor` with [batch, x_dim, x_dim, 1]): MNIST dataset with batch size.\n",
    "        \"\"\"\n",
    "        with tf.variable_op_scope('Read_mnist'):\n",
    "            iterator = dataset.make_one_shot_iterator\n",
    "\n",
    "            self.mnist = iterator.get_next()\n",
    "            self.mnist = tf.cast(self.mnist, dtype=tf.float32)\n",
    "            self.mnist = tf.expand_dims(self.mnist, axis=3)\n",
    "        \n",
    "        return self.mnist\n",
    "    \n",
    "    def Generator(self, random_z, is_training = True, reuse = False):\n",
    "        \"\"\"Generator setup.\n",
    "\n",
    "        Args:\n",
    "          random_z (`2-rank Tensor` with [batch_size, z_dim]):\n",
    "              latent vector which size is generally 100 dim.\n",
    "          is_training (`bool`): whether training mode or test mode.\n",
    "          reuse (`bool`): whether variable reuse or not.\n",
    "\n",
    "        Returns:\n",
    "          generated_data (`4-rank Tensor` with [batch_size, h, w, c])\n",
    "              generated images from random vector z.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('Generator', reuse=reuse):\n",
    "            batch_norm_params = {'decay' :0.9, 'epsilion':0.0001,\n",
    "                                'is_training' : is_training, 'scope':'batch_norm'}\n",
    "            with slim.arg_scope([slim.conv2d_transpose],\n",
    "                               kernel_size = [4,4], stride = [2,2],\n",
    "                               normalizer_fn = slim.batch_norm,\n",
    "                               normalizer_params = batch_norm_params):\n",
    "                #Use full conv2d_transpose intead of projection and reshape\n",
    "                #random_z = [batch_size, z_dim]\n",
    "                self.inputs = random_z\n",
    "                #inputs = 1 x 1 x 100 dim ??\n",
    "                #outputs = 3 x 3 x 256 dim\n",
    "                self.layer1 = slim.conv2d_transpose(inputs=self.inputs, num_outputs=256, padding= 'VALID', scope='layer1'\n",
    "                                                   pa)\n",
    "                \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
