{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN with MNIST\n",
    "\n",
    "* MNIST data를 가지고 **Conditional GAN**를 `tf.contrib.slim`을 이용하여 만들어보자.\n",
    "  * 참고 [TensorFlow slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)\n",
    "* based on dcgan\n",
    "* Original paper: Mehdi Mirza and Simon Osindero, \"Conditional Generative Adversarial Nets\"\n",
    "* reference code\n",
    "  * [ilguyi's gans.tensorflow.slim](https://github.com/ilguyi/gans.tensorflow.slim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#tf.set_random_seed(219)\n",
    "#np.random.seed(219)\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "train_dir = 'train/cgan/exp1/'\n",
    "max_epochs = 20 * 2 # effectively 30 epoch due to twice call the same data\n",
    "#max_epochs = 20\n",
    "save_epochs = 10\n",
    "summary_steps = 2500\n",
    "print_steps = 1\n",
    "batch_size = 64\n",
    "learning_rate_D = 0.0002\n",
    "learning_rate_G = 0.001\n",
    "k = 1 # the number of step of learning D before learning G\n",
    "num_samples = 10 # the number of class on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), _ = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int)\n",
    "train_labels = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`\n",
    "\n",
    "### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=max_epochs)\n",
    "#train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "train_dataset = train_dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN(object):\n",
    "  \"\"\"Conditional Generative Adversarial Networks\n",
    "  implementation based on http://arxiv.org/abs/1411.1784\n",
    "  \n",
    "  \"Conditional Generative Adversarial Nets\"\n",
    "  Mehdi Mirza, Simon Osindero\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, mode, train_dataset, test_dataset=None):\n",
    "    \"\"\"Basic setup.\n",
    "    \n",
    "    Args:\n",
    "      mode (`string`): \"train\" or \"generate\".\n",
    "      train_dataset (`tf.data.Dataset`): train_dataset.\n",
    "      test_dataset (`tf.data.Dataset`): test_dataset.\n",
    "    \"\"\"\n",
    "    assert mode in [\"train\", \"generate\"]\n",
    "    self.mode = mode\n",
    "    \n",
    "    # hyper-parameters for model\n",
    "    self.x_dim = 28\n",
    "    self.z_dim = 100\n",
    "    self.y_dim = 10 # for number of class on MNIST\n",
    "    self.batch_size = batch_size\n",
    "    self.num_samples = num_samples\n",
    "    self.train_dataset = train_dataset\n",
    "    self.test_dataset = test_dataset\n",
    "    \n",
    "    # Global step Tensor.\n",
    "    self.global_step = None\n",
    "    \n",
    "    print('The mode is %s.' % self.mode)\n",
    "    print('complete initializing model.')\n",
    "    \n",
    "    \n",
    "  def build_random_z_inputs(self):\n",
    "    \"\"\"Build a vector random_z in latent space.\n",
    "    \n",
    "    Returns:\n",
    "      self.random_z (`4-rank Tensor` with [batch_size, 1, 1, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "      self.sample_random_z (`4-rank Tensor` with [num_samples, 1, 1, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "    \"\"\"\n",
    "    # 여기를 채워 넣으세요\n",
    "    # Setup variable of random vector z\n",
    "    with tf.variable_scope('random_z'):\n",
    "      self.random_z = tf.random_uniform([self.batch_size, 1, 1, self.z_dim],\n",
    "                                        minval=-1.0, maxval=1.0)\n",
    "      self.sample_random_z = tf.random_uniform([self.num_samples, 1, 1, self.z_dim],\n",
    "                                               minval=-1.0, maxval=1.0)\n",
    "\n",
    "    return self.random_z, self.sample_random_z\n",
    "  \n",
    "  \n",
    "  def read_MNIST(self, dataset):\n",
    "    \"\"\"Read MNIST dataset and create a conditional vector c.\n",
    "    \n",
    "    Args:\n",
    "      dataset (`tf.data.Dataset` format): MNIST dataset.\n",
    "      \n",
    "    Returns:\n",
    "      self.mnist (`4-rank Tensor` with [batch, x_dim, x_dim, 1]): MNIST dataset with batch size.\n",
    "      self.condition (`4-rank Tensor` with [batch, 1, 1, y_dim]): MNIST lable dataset with batch size.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('mnist'):\n",
    "      iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "      self.mnist, self.condition = iterator.get_next()\n",
    "      self.mnist = tf.cast(self.mnist, dtype = tf.float32)\n",
    "      self.mnist = tf.expand_dims(self.mnist, axis=3)\n",
    "      self.condition = tf.one_hot(self.condition, depth=self.y_dim)\n",
    "      self.condition = tf.reshape(self.condition, shape=[-1, 1, 1, self.y_dim])\n",
    "      self.condition = tf.cast(self.condition, dtype = tf.float32)\n",
    "      \n",
    "    return self.mnist, self.condition\n",
    "\n",
    "\n",
    "  def Generator(self, random_z, condition, is_training=True, reuse=False):\n",
    "    \"\"\"Generator setup.\n",
    "    \n",
    "    Args:\n",
    "      random_z (`4-rank Tensor` with [batch_size, 1, 1, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "      condition (`4-rank Tensor` with [batch_size, 1, 1, y_dim]):\n",
    "          conditional vector which size is the number of class on MNIST.\n",
    "      is_training (`bool`): whether training mode or test mode.\n",
    "      reuse (`bool`): whether variable reuse or not.\n",
    "      \n",
    "    Returns:\n",
    "      generated_data (`4-rank Tensor` with [batch_size, h, w, c])\n",
    "          generated images from random vector z.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Generator', reuse=reuse) as scope:\n",
    "      batch_norm_params = {'decay': 0.9,\n",
    "                           'epsilon': 0.001,\n",
    "                           'is_training': is_training,\n",
    "                           'scope': 'batch_norm'}\n",
    "      with slim.arg_scope([slim.conv2d_transpose],\n",
    "                          kernel_size=[4, 4],\n",
    "                          stride=[2, 2],\n",
    "                          normalizer_fn=slim.batch_norm,\n",
    "                          normalizer_params=batch_norm_params):\n",
    "        # 여기를 채워 넣으세요\n",
    "        # Use full conv2d_transpose instead of projection and reshape\n",
    "        # random_z: 100 dim\n",
    "        # condition: 10 dim (for MNIST)\n",
    "        # inputs = random_z + condition\n",
    "        self.inputs = tf.concat([random_z, condition], axis=3)\n",
    "        # inputs: 1 x 1 x 110 dim\n",
    "        # outputs: 3 x 3 x 256 dim\n",
    "        self.layer1 = slim.conv2d_transpose(inputs=self.inputs,\n",
    "                                            num_outputs=256,\n",
    "                                            kernel_size=[3, 3],\n",
    "                                            padding='VALID',\n",
    "                                            scope='layer1')\n",
    "        # inputs: 3 x 3 x 256 dim\n",
    "        # outputs: 7 x 7 x 128 dim\n",
    "        self.layer2 = slim.conv2d_transpose(inputs=self.layer1,\n",
    "                                            num_outputs=128,\n",
    "                                            kernel_size=[3, 3],\n",
    "                                            padding='VALID',\n",
    "                                            scope='layer2')\n",
    "        # inputs: 7 x 7 x 128 dim\n",
    "        # outputs: 14 x 14 x 64 dim\n",
    "        self.layer3 = slim.conv2d_transpose(inputs=self.layer2,\n",
    "                                            num_outputs=64,\n",
    "                                            scope='layer3')\n",
    "        # inputs: 14 x 14 x 64 dim\n",
    "        # outputs: 28 x 28 x 1 dim\n",
    "        self.layer4 = slim.conv2d_transpose(inputs=self.layer3,\n",
    "                                            num_outputs=1,\n",
    "                                            normalizer_fn=None,\n",
    "                                            activation_fn=tf.sigmoid,\n",
    "                                            scope='layer4')\n",
    "        # generated_data = outputs: 28 x 28 x 1 dim\n",
    "        generated_data = self.layer4\n",
    "\n",
    "        return generated_data\n",
    "    \n",
    "    \n",
    "  def Discriminator(self, data, condition, reuse=False):\n",
    "    \"\"\"Discriminator setup.\n",
    "    \n",
    "    Args:\n",
    "      data (`4-rank Tensor` with [batch_size, x_dim, x_dim, 1]): MNIST real data.\n",
    "      condition (`4-rank Tensor` with [batch_size, 1, 1, y_dim]):\n",
    "          conditional vector which size is the number of class on MNIST.\n",
    "      reuse (`bool`): whether variable reuse or not.\n",
    "      \n",
    "    Returns:\n",
    "      logits (`1-rank Tensor` with [batch_size]): logits of data.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator', reuse=reuse) as scope:\n",
    "      batch_norm_params = {'decay': 0.9,\n",
    "                           'epsilon': 0.001,\n",
    "                           'scope': 'batch_norm'}\n",
    "      with slim.arg_scope([slim.conv2d],\n",
    "                          kernel_size=[4, 4],\n",
    "                          stride=[2, 2],\n",
    "                          activation_fn=tf.nn.leaky_relu,\n",
    "                          normalizer_fn=slim.batch_norm,\n",
    "                          normalizer_params=batch_norm_params):\n",
    "        # 여기를 채워 넣으세요\n",
    "        # data: 28 x 28 x 1 dim\n",
    "        # condition: 10 dim (for MNIST)\n",
    "        # inputs = data + condition\n",
    "        self.inputs = tf.concat([data,\n",
    "                                 condition * tf.ones([self.batch_size,\n",
    "                                                      self.x_dim, self.x_dim,\n",
    "                                                      self.y_dim])], axis=3)\n",
    "        # inputs: 28 x 28 x (1 + 10) dim\n",
    "        # outputs: 14 x 14 x 64 dim\n",
    "        self.layer1 = slim.conv2d(inputs=self.inputs,\n",
    "                                  num_outputs=64,\n",
    "                                  normalizer_fn=None,\n",
    "                                  scope='layer1')\n",
    "        # inputs: 14 x 14 x 64 dim\n",
    "        # outputs: 7 x 7 x 128 dim\n",
    "        self.layer2 = slim.conv2d(inputs=self.layer1,\n",
    "                                  num_outputs=128,\n",
    "                                  scope='layer2')\n",
    "        # inputs: 7 x 7 x 128 dim\n",
    "        # outputs: 3 x 3 x 256 dim\n",
    "        self.layer3 = slim.conv2d(inputs=self.layer2,\n",
    "                                  num_outputs=256,\n",
    "                                  kernel_size=[3, 3],\n",
    "                                  padding='VALID',\n",
    "                                  scope='layer3')\n",
    "        # inputs: 3 x 3 x 256 dim\n",
    "        # outputs: 1 x 1 x 1 dim\n",
    "        self.layer4 = slim.conv2d(inputs=self.layer3,\n",
    "                                  num_outputs=1,\n",
    "                                  kernel_size=[3, 3],\n",
    "                                  stride=[1, 1],\n",
    "                                  padding='VALID',\n",
    "                                  normalizer_fn=None,\n",
    "                                  activation_fn=None,\n",
    "                                  scope='layer4')\n",
    "        # logits = layer4: 1 x 1 x 1 dim -> 1 dim\n",
    "        discriminator_logits = tf.squeeze(self.layer4, axis=[1, 2])\n",
    "\n",
    "        return discriminator_logits\n",
    "    \n",
    "    \n",
    "  def setup_global_step(self):\n",
    "    \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "    if self.mode == \"train\":\n",
    "      self.global_step = tf.train.get_or_create_global_step()\n",
    "      \n",
    "      print('complete setup global_step.')\n",
    "      \n",
    "      \n",
    "  def GANLoss(self, logits, is_real=True, scope=None):\n",
    "    \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "    \n",
    "    Args:\n",
    "      logits (`1-rank Tensor`): logits.\n",
    "      is_real (`bool`): True means `1` labeling, False means `0` labeling.\n",
    "      \n",
    "    Returns:\n",
    "      loss (`0-randk Tensor): the standard GAN loss value. (binary_cross_entropy)\n",
    "    \"\"\"\n",
    "    if is_real:\n",
    "      labels = tf.ones_like(logits)\n",
    "    else:\n",
    "      labels = tf.zeros_like(logits)\n",
    "\n",
    "    # 여기를 채워 넣으세요\n",
    "    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels,\n",
    "                                           logits=logits,\n",
    "                                           scope=scope)\n",
    "\n",
    "    return loss\n",
    "\n",
    "      \n",
    "  def build(self):\n",
    "    \"\"\"Creates all ops for training or generate.\"\"\"\n",
    "    self.setup_global_step()\n",
    "    \n",
    "    if self.mode == \"generate\":\n",
    "      pass\n",
    "    \n",
    "    else:\n",
    "      # generating random vector\n",
    "      self.random_z, self.sample_random_z = self.build_random_z_inputs()\n",
    "      # read dataset and create conditional vector\n",
    "      self.real_data, self.condition = self.read_MNIST(self.train_dataset)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # generating images from Generator() via random vector z\n",
    "      self.generated_data = self.Generator(self.random_z, self.condition)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # discriminating real data by Discriminator()\n",
    "      self.real_logits = self.Discriminator(self.real_data, self.condition)\n",
    "      # discriminating fake data (generated)_images) by Discriminator()\n",
    "      self.fake_logits = self.Discriminator(self.generated_data, self.condition, reuse=True)\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # losses of real with label \"1\"\n",
    "      self.loss_real = self.GANLoss(logits=self.real_logits, is_real=True, scope='loss_D_real')\n",
    "      # losses of fake with label \"0\"\n",
    "      self.loss_fake = self.GANLoss(logits=self.fake_logits, is_real=False, scope='loss_D_fake')\n",
    "      \n",
    "      # losses of Discriminator\n",
    "      with tf.variable_scope('loss_D'):\n",
    "        self.loss_Discriminator = self.loss_real + self.loss_fake\n",
    "      # 여기를 채워 넣으세요\n",
    "      # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "      self.loss_Generator = self.GANLoss(logits=self.fake_logits, is_real=True, scope='loss_G')\n",
    "      \n",
    "      # 여기를 채워 넣으세요\n",
    "      # Separate variables for each function\n",
    "      self.D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n",
    "      self.G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n",
    "      \n",
    "      \n",
    "      # generating images for sample\n",
    "      self.sample_condition = tf.eye(self.y_dim)\n",
    "      self.sample_condition = tf.reshape(self.sample_condition, [-1, 1, 1, self.y_dim])\n",
    "      self.sample_data = self.Generator(self.sample_random_z, self.sample_condition, is_training=False, reuse=True)\n",
    "      \n",
    "      # write summaries\n",
    "      # Add loss summaries\n",
    "      tf.summary.scalar('losses/loss_Discriminator', self.loss_Discriminator)\n",
    "      tf.summary.scalar('losses/loss_Generator', self.loss_Generator)\n",
    "      \n",
    "      # Add histogram summaries\n",
    "      for var in self.D_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      for var in self.G_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      \n",
    "      # Add image summaries\n",
    "      tf.summary.image('random_images', self.generated_data, max_outputs=4)\n",
    "      tf.summary.image('real_images', self.real_data)\n",
    "      \n",
    "    print('complete model build.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_data(sample_data, max_print=num_samples):\n",
    "  print_images = sample_data[:max_print,:]\n",
    "  print_images = print_images.reshape([max_print, 28, 28])\n",
    "  print_images = print_images.swapaxes(0, 1)\n",
    "  print_images = print_images.reshape([28, max_print * 28])\n",
    "  \n",
    "  plt.figure(figsize=(max_print, 1))\n",
    "  plt.axis('off')\n",
    "  plt.imshow(print_images, cmap='gray')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CGAN(mode=\"train\", train_dataset=train_dataset)\n",
    "model.build()\n",
    "\n",
    "# show info for trainable variables\n",
    "t_vars = tf.trainable_variables()\n",
    "slim.model_analyzer.analyze_vars(t_vars, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_D = tf.train.AdamOptimizer(learning_rate=learning_rate_D, beta1=0.5)\n",
    "opt_G = tf.train.AdamOptimizer(learning_rate=learning_rate_G, beta1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 채워 넣으세요\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Discriminator')):\n",
    "  opt_D_op = opt_D.minimize(model.loss_Discriminator, var_list=model.D_vars)\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Generator')):\n",
    "  opt_G_op = opt_G.minimize(model.loss_Generator, global_step=model.global_step,\n",
    "                            var_list=model.G_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign `tf.summary.FileWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_location = train_dir\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.train.Saver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  tf.logging.info('Start Session.')\n",
    "  \n",
    "  num_examples = len(train_data)\n",
    "  num_batches_per_epoch = int(num_examples / batch_size)\n",
    "  \n",
    "  # save loss values for plot\n",
    "  loss_history = []\n",
    "  pre_epochs = 0\n",
    "  while True:\n",
    "    try:\n",
    "      start_time = time.time()\n",
    "      \n",
    "      for _ in range(k):\n",
    "        _, loss_D = sess.run([opt_D_op, model.loss_Discriminator])\n",
    "      _, global_step_, loss_G = sess.run([opt_G_op,\n",
    "                                          model.global_step,\n",
    "                                          model.loss_Generator])\n",
    "      \n",
    "      epochs = global_step_ * batch_size / float(num_examples)\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      if global_step_ % print_steps == 0:\n",
    "        examples_per_sec = batch_size / float(duration)\n",
    "        print(\"Epochs: {:.2f} global_step: {} loss_D: {:.3f} loss_G: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                  epochs, global_step_, loss_D, loss_G, examples_per_sec, duration))\n",
    "\n",
    "        loss_history.append([epochs, loss_D, loss_G])\n",
    "\n",
    "        # print sample data\n",
    "        sample_data = sess.run(model.sample_data)\n",
    "        print_sample_data(sample_data)\n",
    "\n",
    "      # write summaries periodically\n",
    "      if global_step_ % summary_steps == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        train_writer.add_summary(summary_str, global_step=global_step_)\n",
    "\n",
    "      # save model checkpoint periodically\n",
    "      if int(epochs) % save_epochs == 0  and  pre_epochs != int(epochs):\n",
    "        tf.logging.info('Saving model with global step {} (= {} epochs) to disk.'.format(global_step_, int(epochs)))\n",
    "        saver.save(sess, train_dir + 'model.ckpt', global_step=global_step_)\n",
    "        pre_epochs = int(epochs)\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      tf.logging.info('Saving model with global step {} (= {} epochs) to disk.'.format(global_step_, int(epochs)))\n",
    "      saver.save(sess, train_dir + 'model.ckpt', global_step=global_step_)\n",
    "      break\n",
    "      \n",
    "  tf.logging.info('complete training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.asarray(loss_history)\n",
    "\n",
    "plt.plot(loss_history[:,0], loss_history[:,1], label='loss_D')\n",
    "plt.plot(loss_history[:,0], loss_history[:,2], label='loss_G')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
