{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "Simple example for Seq2Seq (Machine Translation) by Encoder RNN and Decoder RNN.\n",
    "\n",
    "### Seq2Seq by Encoder RNN and Decoder RNN\n",
    "- Creating the **data pipeline** with `tf.data`\n",
    "- Preprocessing word sequences (variable input sequence length) using `padding technique` by `user function (pad_seq)`\n",
    "- Using `tf.nn.embedding_lookup` for getting vector of tokens (eg. word, character)\n",
    "- Training **many to many classification** with `tf.contrib.seq2seq.sequence_loss`\n",
    "- Masking unvalid token with `tf.sequence_mask`\n",
    "- Using `tf.contrib.seq2seq.dynamic_decode`\n",
    "- Training with `tf.contrib.seq2seq.TrainingHelper`\n",
    "- Translating with `tf.contriv.seq2seq.GreedyEmbeddingHelper`\n",
    "- Creating the model as **Class**\n",
    "- Reference\n",
    "    - https://github.com/golbin/TensorFlow-Tutorials/blob/master/10%20-%20RNN/03%20-%20Seq2Seq.py\n",
    "    - https://github.com/HiJiGOO/tf_nmt_tutorial\n",
    "    - https://github.com/hccho2/RNN-Tutorial\n",
    "    - https://github.com/j-min/tf_tutorial_plus/tree/master/RNN_seq2seq/contrib_seq2seq\n",
    "    - https://gist.github.com/ilblackdragon/c92066d9d38b236a21d5a7b729a10f12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare example data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'I': 1, 'a': 2, 'changing': 3, 'deep': 4, 'difficult': 5, 'fast': 6, 'feel': 7, 'for': 8, 'framework': 9, 'hungry': 10, 'is': 11, 'learning': 12, 'tensorflow': 13, 'very': 14}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# word dic for sentences\n",
    "source_words = []\n",
    "for elm in sources:\n",
    "    source_words += elm\n",
    "source_words = list(set(source_words))\n",
    "source_words.sort()\n",
    "source_words = ['<pad>'] + source_words\n",
    "\n",
    "source_dic = {word : idx for idx, word in enumerate(source_words)}\n",
    "print(source_dic)\n",
    "print(len(source_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: 'I',\n",
       " 2: 'a',\n",
       " 3: 'changing',\n",
       " 4: 'deep',\n",
       " 5: 'difficult',\n",
       " 6: 'fast',\n",
       " 7: 'feel',\n",
       " 8: 'for',\n",
       " 9: 'framework',\n",
       " 10: 'hungry',\n",
       " 11: 'is',\n",
       " 12: 'learning',\n",
       " 13: 'tensorflow',\n",
       " 14: 'very'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_idx_dic = {elm[1] : elm[0] for elm in source_dic.items()}\n",
    "source_idx_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<start>': 1, '<end>': 2, '고프다': 3, '나는': 4, '딥러닝을': 5, '매우': 6, '배가': 7, '변화한다': 8, '빠르게': 9, '어렵다': 10, '위한': 11, '텐서플로우는': 12, '프레임워크이다': 13}\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# word dic for translations\n",
    "target_words = []\n",
    "for elm in targets:\n",
    "    target_words += elm\n",
    "target_words = list(set(target_words))\n",
    "target_words.sort()\n",
    "target_words =  ['<pad>']+ ['<start>'] + ['<end>'] + \\\n",
    "                    target_words # 번역문의 시작과 끝을 알리는 'start', 'end' token 추가\n",
    "\n",
    "target_dic = {word : idx for idx, word in enumerate(target_words)}\n",
    "print(target_dic)\n",
    "print(len(target_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<start>',\n",
       " 2: '<end>',\n",
       " 3: '고프다',\n",
       " 4: '나는',\n",
       " 5: '딥러닝을',\n",
       " 6: '매우',\n",
       " 7: '배가',\n",
       " 8: '변화한다',\n",
       " 9: '빠르게',\n",
       " 10: '어렵다',\n",
       " 11: '위한',\n",
       " 12: '텐서플로우는',\n",
       " 13: '프레임워크이다'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_idx_dic = {elm[1] : elm[0] for elm in target_dic.items()}\n",
    "target_idx_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pad_seq function for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq_enc(sequences, max_len, dic):\n",
    "    seq_len = []\n",
    "    seq_indices = []\n",
    "    for seq in sequences:\n",
    "        seq_len.append(len(seq))\n",
    "        seq_idx = [dic.get(word) for word in seq]\n",
    "        seq_idx += (max_len - len(seq_idx)) * [dic.get('<pad>')] \n",
    "        seq_indices.append(seq_idx)        \n",
    "    return seq_len, seq_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq_dec(sequences, max_len, dic):\n",
    "    seq_input_len = []\n",
    "    seq_input_indices = []\n",
    "    seq_target_indices = []\n",
    "    \n",
    "    # for decoder input\n",
    "    for seq in sequences:\n",
    "        seq_input_idx = [dic.get('<start>')] + [dic.get(token) for token in seq]\n",
    "        seq_input_len.append(len(seq_input_idx))\n",
    "        seq_input_idx += (max_len - len(seq_input_idx)) * [dic.get('<pad>')] \n",
    "        seq_input_indices.append(seq_input_idx)\n",
    "        \n",
    "    # for decoder output\n",
    "    for seq in sequences:\n",
    "        seq_target_idx = [dic.get(token) for token in seq] + [dic.get('<end>')]\n",
    "        seq_target_idx += (max_len - len(seq_target_idx)) * [dic.get('<pad>')]\n",
    "        seq_target_indices.append(seq_target_idx)\n",
    "        \n",
    "    return seq_input_len, seq_input_indices, seq_target_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] (4, 10)\n",
      "[[1, 7, 10, 0, 0, 0, 0, 0, 0, 0],\n",
      " [13, 11, 14, 5, 0, 0, 0, 0, 0, 0],\n",
      " [13, 11, 2, 9, 8, 4, 12, 0, 0, 0],\n",
      " [13, 11, 14, 6, 3, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# for encoder\n",
    "source_max_len = 10\n",
    "X_length, X_indices = pad_seq_enc(sequences = sources, max_len = source_max_len, dic = source_dic)\n",
    "print(X_length, np.shape(X_indices))\n",
    "pprint(X_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 5, 5]\n",
      "[[1, 4, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 6, 10, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 5, 11, 13, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 6, 9, 8, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[4, 7, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 6, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 5, 11, 13, 2, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 6, 9, 8, 2, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# for decoder\n",
    "target_max_len = 12\n",
    "y_length, y_input_indices, y_target_indices = pad_seq_dec(sequences = targets, max_len = target_max_len,\n",
    "                                                             dic = target_dic)\n",
    "pprint(y_length) #실제 길이\n",
    "pprint(y_input_indices) #start token + dic\n",
    "pprint(y_target_indices) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-74e9467dbb01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 's_indices' is not defined"
     ]
    }
   ],
   "source": [
    "s_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = tf.get_variable(initializer=s, dtype=tf.float32, name='ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SimpleNMT\n",
    "Encoder RNN, Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNMT:\n",
    "    def __init__(self, s_len, s_indices, t_len, t_input_indices, t_output_indices,\n",
    "                 t_max_len = target_max_len, s_dic = source_dic, t_dic = target_dic,\n",
    "                 n_of_classes = len(target_dic), hidden_dim = 16):\n",
    "        \n",
    "        with tf.variable_scope('input_layer'):\n",
    "            # s : source, t : target\n",
    "            self._s_len = s_len\n",
    "            self._s_indices = s_indices\n",
    "            self._t_len = t_len\n",
    "            self._t_input_indices = t_input_indices\n",
    "            self._t_output_indices = t_output_indices\n",
    "            self._s_dic = s_dic #source_dic\n",
    "            self._t_dic = t_dic #target_dic\n",
    "            self._t_max_len = target_max_len #만약에 end token이 나오지 않을때, 끝내기 위해 사용\n",
    "            \n",
    "            s_embeddings = tf.eye(num_rows=len(s_dic), dtype=tf.float32, name='tf.eye') \n",
    "            #s_embeddings = tf.eye(num_rows=s_len, dtype=tf.float32, name='tf.eye') # tf.eye를 이용하세요\n",
    "            s_embeddings = tf.get_variable(initializer= s_embeddings, dtype=tf.float32, trainable= False,\n",
    "                                           name='s_embeddings')\n",
    "            # tf.get_variable을 이용하세요\n",
    "            s_batch = tf.nn.embedding_lookup(self.s_embeddings, ids= self.s_indices) # tf.nn.embedding_lookup을 이용하세요.\n",
    "        \n",
    "        with tf.variable_scope('encoder'):\n",
    "            \n",
    "            enc_cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_dim, dtype=tf.float32, name='enc_cell')\n",
    "            # tf.contrib.rnn.BasicRNNCell을 이용하세요\n",
    "            _, enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs= s_batch, sequence_length=self._s_len,\n",
    "                                             dtype= tf.float32)\n",
    "            # tf.nn.dynamic_rnn을 이용하세요, rnn_cell을 실행하는 명령어\n",
    "            # {batch_size, max_sequence_length, input_dim}\n",
    "            # {max_sequence_legnth}??\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('pipe'): #decoder_input \n",
    " \n",
    "            t_embeddings = tf.eye(num_rows=len(t_dic), dtype=tf.float32, name='tf.eye') \n",
    "            #s_embeddings = tf.eye(num_rows=s_len, dtype=tf.float32, name='tf.eye') # tf.eye를 이용하세요\n",
    "            t_embeddings = tf.get_variable(initializer= t_embeddings, dtype=tf.float32, trainable= False,\n",
    "                                           name='t_embeddings')\n",
    "            # tf.get_variable을 이용하세요\n",
    "            t_batch = tf.nn.embedding_lookup(self.t_embeddings, ids= self.t_indices) # tf.nn.embedding_lookup을 이용하세요.\n",
    "            \n",
    "            tokens = tf.ones_like(tensor = self._s_len, dtype = tf.int32) # idx 1 start_token\n",
    "            batch_size = tf.reduce_sum(tokens)\n",
    "            tf_max_len = tf.tile(input = [self._t_max_len], mutiples = [batch_size])\n",
    "            tf.tile(input=[self.t_dic.get('<start.')], multiples=[batch_size])\n",
    "            \n",
    "            \n",
    "            #tokens = batch_size, mini_batch_size만큼 만들기 위해서 사용\n",
    "            tr_tokens = tf.map_fn(lambda elm : tf.multiply(elm, self._t_max_len), tokens)\n",
    "            trans_tokens = tokens\n",
    "        \n",
    "        with tf.variable_scope('decoder'):\n",
    "            \n",
    "            dec_cell = tf.contrib.rnn.BasicRNNCell(num_units= hidden_dim, dtype=tf.float32, name='enc_cell') # tf.contrib.rnn.BasicRNNCell를 이용하세요\n",
    "        \n",
    "            tf.contrib.seq2seq.LuongAttention(num_units= hidden_dim, memory= enc_outputs, memory_sequence_length= self._s_len)\n",
    "            \n",
    "            attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell=, dec_cell, attention_mechanism= attn, initial_cell_state= enc_state)\n",
    "            \n",
    "            zero_cell = tf.\n",
    "            score_cell = tf.contrib.rnn.OutputProjectionWrapper(cell=dec_cell, output_size=n_of_classes) # tf.contrib.rnn.OutputProjectionWrapper를 이용하세요\n",
    "            \n",
    "            \n",
    "            with tf.variable_scope('training'):\n",
    "            \n",
    "                tr_helper = tf.contrib.seq2seq.TrainingHelper(inputs= t_batch, sequence_length= tr_tokens) # tf.contrib.seq2seq.TrainingHelper를 이용하세요\n",
    "                tr_decoder = tf.contrib.seq2seq.BasicDecoder(cell=score_cell,helper=tr_helper, initial_state= enc_state) # tf.contrib.seq2seq.BasicDecoder를 이용하세요\n",
    "                self._tr_outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(decoder=tr_decoder,impute_finished= True,\n",
    "                                                                         maximum_iterations= sefl._t_max_len) # tf.contrib.seq2seq.dynamic_decode를 이용하세요\n",
    "                \n",
    "            with tf.variable_scope('translation'):\n",
    "                \n",
    "                trans_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=t_embeddings, start_tokens=start_tokens,\n",
    "                                                                        end_token=sefl._t_output_indices.get({'get'})) # tf.contrib.seq2seq.GreedyEmbeddingHelper를 이용하세요\n",
    "                #max_padding 5, sequence_length 10 -> 5가 나온다??\n",
    "                #ex) \n",
    "                #end_token에 index가 하나, \n",
    "                trans_decoder = tf.contrib.seq2seq.BasicDecoder(cell= score_cell, helper= trans_helper, initial_state=enc_state) # tf.contrib.seq2seq.BasicDecoder를 이용하세요\n",
    "                self._trans_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=trans_decoder, impute_finished=True,maximum_iterations= sefl._t_max_len*3) # tf.contrib.seq2seq.dynamic_decode를 이용하세요\n",
    "                \n",
    "        with tf.variable_scope('seq2seq_loss'):\n",
    "            \n",
    "            masking = tf.sequence_mask(lengths=self._t_len, maxlen=self._t_max_len, dtype=tf.float32) # tf.sequence_mask\n",
    "            self.seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits=tr_outputs.rnn.output, targets = self._t_output_indices,\n",
    "                                                                weights = masking) # tf.contrib.seq2seq.sequence_loss를 이용하세요\n",
    "            \n",
    "    def translate(self, sess, s_len, s_indices):\n",
    "        feed_translation = {self._s_len : s_len, self._s_indices : s_indices}\n",
    "        return sess.run(self._trans_outputs.sample_id, feed_dict = feed_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model of SimpleNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter#\n",
    "lr = .003\n",
    "epochs = 200\n",
    "batch_size = 2\n",
    "total_step = int(np.shape(X_indices)[0] / batch_size)\n",
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data pipeline with tf.data\n",
    "tr_dataset = None # tf.data.Dataset.from_tensor_slices를 이용하세요\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size = 20)\n",
    "tr_dataset = tr_dataset.batch(batch_size = batch_size)\n",
    "tr_iterator = tr_dataset.make_initializable_iterator()\n",
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_length_mb, X_indices_mb, y_length_mb, y_input_indices_mb, y_target_indices_mb = tr_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_nmt = SimpleNMT(s_len = X_length_mb, s_indices  = X_indices_mb,\n",
    "                    t_len = y_length_mb, t_input_indices = y_input_indices_mb,\n",
    "                    t_output_indices = y_target_indices_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat training op and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create training op\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "training_op = opt.minimize(loss = sim_nmt.seq2seq_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tr_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    sess.run(tr_iterator.initializer)\n",
    "    try:\n",
    "        while True:\n",
    "            _, tr_loss = sess.run([training_op, sim_nmt.seq2seq_loss])\n",
    "            avg_tr_loss += tr_loss\n",
    "            tr_step += 1\n",
    "\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    \n",
    "    avg_tr_loss /= tr_step\n",
    "    tr_loss_hist.append(avg_tr_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = sim_nmt.translate(sess = sess, s_len = X_length, s_indices = X_indices)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 문장\n",
    "originals = list(map(lambda elm : [target_idx_dic.get(idx) for idx in elm], y_target_indices))\n",
    "for original in originals:\n",
    "    print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 넣은 번역문장\n",
    "translations = list(map(lambda elm : [target_idx_dic.get(idx) for idx in elm], yhat))\n",
    "for translation in translations:\n",
    "    print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
