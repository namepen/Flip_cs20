{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"style.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"8wkHVou9NyNU","colab_type":"text"},"cell_type":"markdown","source":["http://sanghyukchun.github.io/92/#92-reconst-img\n","    \n"]},{"metadata":{"id":"ttLcyT3_N3PL","colab_type":"code","outputId":"97a73356-933a-4c20-ab8b-76e097642d6e","executionInfo":{"status":"ok","timestamp":1543390436673,"user_tz":-540,"elapsed":155546,"user":{"displayName":"이은용","photoUrl":"","userId":"18068618174645565873"}},"colab":{"base_uri":"https://localhost:8080/","height":371}},"cell_type":"code","source":["#Connected google drive\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package libfuse2:amd64.\n","(Reading database ... 22298 files and directories currently installed.)\n","Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package fuse.\n","Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking fuse (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package google-drive-ocamlfuse.\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu2~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu2~ubuntu18.04.1) ...\n","Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Setting up fuse (2.9.7-1ubuntu1) ...\n","Setting up google-drive-ocamlfuse (0.7.1-0ubuntu2~ubuntu18.04.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"nrb7eV8AN5oE","colab_type":"code","colab":{}},"cell_type":"code","source":["#make data directory\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u8Ov6GtqPaIb","colab_type":"code","colab":{}},"cell_type":"code","source":["!cd drive/tensorflow_flip/input_data/"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KUCXYiqvOYTv","colab_type":"code","outputId":"f85f3cbc-40c2-4bb4-a7d5-69db5861b766","executionInfo":{"status":"ok","timestamp":1543390465415,"user_tz":-540,"elapsed":3608,"user":{"displayName":"이은용","photoUrl":"","userId":"18068618174645565873"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!cd drive/tensorflow_flip/input_data/content.jpg"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/bin/bash: line 0: cd: drive/tensorflow_flip/input_data/content.jpg: Not a directory\n"],"name":"stdout"}]},{"metadata":{"id":"t7PkE3FHNyNj","colab_type":"code","colab":{}},"cell_type":"code","source":["#Setting\n","import os\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from PIL import Image\n","import tensorflow as tf\n","\n","slim = tf.contrib.slim\n","\n","sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"PUJfnUJTNyOj","colab_type":"text"},"cell_type":"markdown","source":["### Hyper Parameters"]},{"metadata":{"id":"9z5wcZJCNyOl","colab_type":"code","colab":{}},"cell_type":"code","source":["input_data_dir = './drive/tensorflow_flip/input_data/'\n","content_image_name = 'content\\ \\(3840e619\\).jpg'\n","style_image_name = 'style\\ \\(292d833f\\).jpg'\n","noise_ratio = 0.05\n","max_L = 1024\n","style_loss_weight = np.array([0.5, 1.0, 1.5, 3.0, 4.0])\n","content_weight = 0.01\n","style_weight = 1.0\n","learning_rate = 0.01\n","max_steps= 10000\n","print_steps = 1000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rJV-ViOINyOt","colab_type":"text"},"cell_type":"markdown","source":["### Load VGG16 graph"]},{"metadata":{"id":"WWTkzLeKNyOu","colab_type":"code","colab":{}},"cell_type":"code","source":["def vgg_16(inputs,\n","           reuse=False,\n","           scope='vgg_16'):\n","  \"\"\"Oxford Net VGG 16-Layers version D Example\n","  \n","  My Note: This code is modified version of vgg_16 which is loacted on `models/research/slim/nets/vgg.py`\n","  Note: All the fully_connected layers have been transformed to conv2d layers.\n","        To use in classification mode, resize input to 224x224.\n","\n","  Args:\n","    inputs: a tensor of size [batch_size, height, width, channels].\n","    reuse: whether or not the model is being reused.\n","    scope: Optional scope for the variables.\n","\n","  Returns:\n","    net: the output of the logits layer (if num_classes is a non-zero integer),\n","      or the input to the logits layer (if num_classes is 0 or None).\n","    end_points: a dict of tensors with intermediate activations.\n","  \"\"\"\n","  with tf.variable_scope(scope, 'vgg_16', [inputs], reuse=reuse) as sc:\n","    end_points_collection = sc.original_name_scope + '_end_points'\n","    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n","                        outputs_collections=end_points_collection):\n","      # 여기를 직접 채워 넣으시면 됩니다.\n","      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3,3], scope='conv1')\n","      net = slim.avg_pool2d(net, [2,2], scope='pool1')\n","      net = slim.repeat(net, 2, slim.conv2d, 128, [3,3], scope='conv2')\n","      net = slim.avg_pool2d(net, [2,2], scope='pool2')\n","      net = slim.repeat(net, 3, slim.conv2d, 256, [3,3], scope='conv3')\n","      net = slim.avg_pool2d(net, [2,2], scope='pool3')\n","      net = slim.repeat(net, 3, slim.conv2d, 512, [3,3], scope='conv4')\n","      net = slim.avg_pool2d(net, [2,2], scope='pool4')\n","      net = slim.repeat(net, 3, slim.conv2d, 512, [3,3], scope='conv5')\n","      net = slim.avg_pool2d(net, [2,2], scope='pool5')\n","\n","      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n","\n","      return net, end_points"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5wfM16ZrNyO3","colab_type":"text"},"cell_type":"markdown","source":["### Read content and style image"]},{"metadata":{"id":"qP4c49nMNyO4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":310},"outputId":"85101723-5341-4612-9cb7-853f5182075a","executionInfo":{"status":"error","timestamp":1543391299154,"user_tz":-540,"elapsed":636,"user":{"displayName":"이은용","photoUrl":"","userId":"18068618174645565873"}}},"cell_type":"code","source":["content_image_ = Image.open(os.path.join(input_data_dir, content_image_name))\n","style_image_ = Image.open(os.path.join(input_data_dir, style_image_name))"],"execution_count":8,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-e34bbed18877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent_image_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_image_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstyle_image_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_image_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2312\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/tensorflow_flip/input_data/content\\\\ \\\\(3840e619\\\\).jpg'"]}]},{"metadata":{"id":"6DUsm_kmG9CC","colab_type":"code","colab":{}},"cell_type":"code","source":["content_image_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0YcFqghFNyPJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def image_resize_with_upper_bound(image, max_L=max_L):\n","  \"\"\"Resize images\n","  \n","  Args:\n","    image: PIL image format\n","    max_L: upper bound of the image size\n","    \n","  Returns:\n","    image: resized image with PIL format\n","    h: resized height\n","    w: resized width\n","    \"\"\"\n","  w, h = image.size\n","  if np.max(np.array([h, w])) > max_L:\n","    if h < w:\n","      h = int(max_L * h / w)\n","      w = max_L\n","    else:\n","      w = int(max_L * w / h)\n","      h = max_L\n","  image = image.resize((h, w))\n","  \n","  return image, h, w"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xBRVXLjvNyPW","colab_type":"code","colab":{}},"cell_type":"code","source":["content_image_, content_image_w, content_image_h = image_resize_with_upper_bound(content_image_)\n","style_image_w, style_image_h = style_image_.size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eTrhRUuSPN4d","colab_type":"code","colab":{}},"cell_type":"code","source":["style_image_h"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5ReQdUKeNyPq","colab_type":"code","colab":{}},"cell_type":"code","source":["# 여기를 직접 채워 넣으시면 됩니다.\n","content_image_p = tf.placeholder(tf.float32, [1, content_image_h, content_image_w, 3], 'content_image_p')\n","style_image_p = tf.placeholder(tf.float32, [1, style_image_h, style_image_w, 3], 'style_image_p')\n","\n","# content_image, style_image를 tf.Variable로 바꾸기 위해 tf.placeholder와 같은 shape의 zero Tensor를 만듦\n","content_image = tf.get_variable(shape=[1, content_image_h, content_image_w, 3], initializer=tf.zeros_initializer(), name='content_image')\n","style_image = tf.get_variable(shape=[1, style_image_h, style_image_w, 3], initializer=tf.zeros_initializer(), name = 'style_image')\n","generated_image = tf.get_variable(name='generated_image',\n","                                  shape=[1, content_image_h, content_image_w, 3],\n","                                  initializer=tf.random_uniform_initializer(minval=-0.2, maxval=0.2))\n","# tf.placeholder를 tf.Variable로 바꿈\n","content_image_op = content_image.assign(content_image_p)\n","style_image_op = style_image.assign(style_image_p)\n","\n","# 초기 이미지는 content_image에 random noise를 섞음\n","generated_image_op = generated_image.assign(generated_image * noise_ratio + \\\n","                                            content_image_p * (1.0 - noise_ratio))\n","# 여기를 직접 채워 넣으시면 됩니다.\n","# generated_image는 매 update 후에 [-1, 1] 사이로 clipping\n","generated_image_clipping = generated_image.assign(tf.clip_by_value(generated_image, clip_value_min=-1,clip_value_max=1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DLaCOYBSNyP3","colab_type":"code","colab":{}},"cell_type":"code","source":["# 여기를 직접 채워 넣으시면 됩니다.\n","_, feature_maps_c = vgg_16(inputs=content_image, reuse=tf.AUTO_REUSE) # input: content_image\n","_, feature_maps_s = vgg_16(inputs=style_image, reuse=tf.AUTO_REUSE) # input: style_image\n","_, feature_maps_g = vgg_16(inputs=generated_image, reuse=tf.AUTO_REUSE) # input: generated_image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LtdSb_CJNyQL","colab_type":"code","colab":{}},"cell_type":"code","source":["feature_maps_c"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uqfecHcnNyQc","colab_type":"text"},"cell_type":"markdown","source":["## Tensorboard Writer"]},{"metadata":{"id":"UMATS-YyNyQe","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:\n","  writer = tf.summary.FileWriter(\"./drive/tensorflow_flip\", sess.graph)\n","  writer.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tAtTXWsINyQn","colab_type":"text"},"cell_type":"markdown","source":["## Build the model"]},{"metadata":{"id":"jJ3RZ5mJNyQo","colab_type":"text"},"cell_type":"markdown","source":["### collecte feature maps\n","\n","* content layers\n","  * `conv4_2`: key name -> 'vgg16/vgg_16/conv4/conv4_2'\n","* style layers\n","  * `conv1_1`: key name -> 'vgg16/vgg_16/conv1/conv1_1'\n","  * `conv2_1`: key name -> 'vgg16/vgg_16/conv2/conv2_1'\n","  * `conv3_1`: key name -> 'vgg16/vgg_16/conv3/conv3_1'\n","  * `conv4_1`: key name -> 'vgg16/vgg_16/conv4/conv4_1'\n","  * `conv5_1`: key name -> 'vgg16/vgg_16/conv5/conv5_1'"]},{"metadata":{"id":"d4PRZXelNyQp","colab_type":"code","colab":{}},"cell_type":"code","source":["content_layers = feature_maps_c['vgg_16/conv4/conv4_2']\n","style_layers = [feature_maps_s['vgg_16/conv1/conv1_1'],\n","                feature_maps_s['vgg_16/conv2/conv2_1'],\n","                feature_maps_s['vgg_16/conv3/conv3_1'],\n","                feature_maps_s['vgg_16/conv4/conv4_1'],\n","                feature_maps_s['vgg_16/conv5/conv5_1']]\n","generated_layers = [feature_maps_g['vgg_16/conv4/conv4_2'],\n","                    feature_maps_g['vgg_16/conv1/conv1_1'],\n","                    feature_maps_g['vgg_16/conv2/conv2_1'],\n","                    feature_maps_g['vgg_16/conv3/conv3_1'],\n","                    feature_maps_g['vgg_16/conv4/conv4_1'],\n","                    feature_maps_g['vgg_16/conv5/conv5_1']]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FOUTTJ4WNyQ3","colab_type":"text"},"cell_type":"markdown","source":["## Content Loss"]},{"metadata":{"id":"5mr0xztGNyQ4","colab_type":"code","colab":{}},"cell_type":"code","source":["?tf.losses.mean_squared_error"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2ktmo-ZQNyQ-","colab_type":"code","colab":{}},"cell_type":"code","source":["def content_loss(P, F, scope):\n","  \"\"\"Calculate the content loss function between\n","  the feature maps of content image and generated image.\n","  \n","  Args:\n","    P: the feature maps of the content image\n","    F: the feature maps of the generated image\n","    scope: scope\n","    \n","  Returns:\n","    loss: content loss (mean squared loss)\n","  \"\"\"\n","  # 여기를 직접 채워 넣으시면 됩니다.\n","  assert F.shape == P.shape #shape이 같을 떄만, 진행                                                                                                                                             ㅓ\n","  loss = tf.reduce_sum(tf.square(P - F)) * 0.5\n","  #loss = tf.losses.mean_squared_error(labels=P, predictions=F, weights=content_weight)\n","  return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J4_Z4JjpNyRE","colab_type":"text"},"cell_type":"markdown","source":["## Style Loss"]},{"metadata":{"id":"AhRL-5GmNyRF","colab_type":"code","colab":{}},"cell_type":"code","source":["def style_loss(style_layers, generated_layers, scope):\n","  \"\"\"Calculate the style loss function between\n","  the gram matrix of feature maps of style image and generated image.\n","  \n","  Args:\n","    style_layers: list of the feature maps of the style image\n","    generated_layers: list of the feature maps of the generated image\n","    scope: scope\n","    \n","  Returns:\n","    loss: style loss (mean squared loss)\n","  \"\"\"\n","  def _style_loss_one_layer(feature_map_s, feature_map_g):\n","    \"\"\"Calculate the style loss for one layer.\n","    \n","    Args:\n","      feature_map_s: the feature map of the style image\n","        - G: the gram matrix of the feature_map_s\n","      feature_map_g: the feature map of the generated image\n","        - A: the gram matrix of the feature_map_g\n","      \n","    Returns:\n","      loss: style loss for one layer (mean squared loss)\n","    \"\"\"\n","    _, h, w, c = feature_map_s.get_shape().as_list()\n","    G = _gram_matrix(feature_map_s)\n","    A = _gram_matrix(feature_map_g)\n","    # 여기를 직접 채워 넣으시면 됩니다.\n","    loss = tf.reduce_sum(tf.square(G - A)) / (4 * (h*w)**2 * c**2)\n","    return loss\n","  \n","  def _gram_matrix(feature_map):\n","    \"\"\"Calculate the gram matrix for the feature map\n","    \n","    Args:\n","      feature_map: 4-rank Tensor [1, height, width, channels]\n","        - F = 2-rank Tensor [h * w, channels]\n","      \n","    Returns:\n","      gram_matrix: 2-rank Tensor [c, c] (F.transpose x F)\n","    \"\"\"\n","    # 여기를 직접 채워 넣으시면 됩니다.\n","    #F = tf.squeeze(feature_map, axis=0) #squeeze는 차원 중 사이즈가 1인 것을 찾아 스칼라값으로 바꿔 해당 차원을 제거한다.\n","    #h, w, c = F.shape\n","    _, h, w, c = feature_map.shape\n","    F = tf.reshape(feature_map, shape=[h*w, c])\n","    return tf.matmul(tf.transpose(F), F)\n","    \n","    \n","  assert len(style_layers) == len(generated_layers)\n","  \n","  loss = 0.0\n","  for i in range(len(style_layers)):\n","    loss_one = _style_loss_one_layer(style_layers[i], generated_layers[i])\n","    loss += loss_one * style_loss_weight[i]\n","\n","  return loss * 0.25"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hJBPBrzBNyRK","colab_type":"text"},"cell_type":"markdown","source":["## Total Loss"]},{"metadata":{"id":"ddXx9xuONyRM","colab_type":"code","colab":{}},"cell_type":"code","source":["loss_c = content_loss(content_layers, generated_layers[0],\n","                      scope='content_loss')\n","print(loss_c)\n","loss_s = style_loss(style_layers, generated_layers[1:],\n","                    scope='style_loss')\n","\n","with tf.variable_scope('total_loss'):\n","  total_loss = content_weight * loss_c + style_weight * loss_s"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gtoZ_GbHNyRT","colab_type":"text"},"cell_type":"markdown","source":["## Define Optimizer"]},{"metadata":{"id":"OPATLFJbNyRU","colab_type":"code","colab":{}},"cell_type":"code","source":["# 여기를 직접 채워 넣으시면 됩니다.\n","train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(total_loss, var_list = generated_image)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xXUZr-MvNyRY","colab_type":"text"},"cell_type":"markdown","source":["## Restore VGG16\n","### weight using `tf.saver.restore`"]},{"metadata":{"id":"vDsUoMywNyRY","colab_type":"text"},"cell_type":"markdown","source":["### Download the VGG16 checkpoint: \n","\n","```\n","$ CHECKPOINT_DIR='./checkpoints'\n","$ mkdir ${CHECKPOINT_DIR}\n","$ cd ${CHECKPOINT_DIR}\n","$ wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n","$ tar -xvf vgg_16_2016_08_28.tar.gz\n","$ rm vgg_16_2016_08_28.tar.gz\n","```"]},{"metadata":{"id":"RfKnvtEpNyRZ","colab_type":"code","colab":{}},"cell_type":"code","source":["CHECKPOINT_DIR='./checkpoints'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RgIROndBNyRe","colab_type":"text"},"cell_type":"markdown","source":["## Preprocessing a iamge"]},{"metadata":{"id":"86_IR_cXNyRl","colab_type":"code","colab":{}},"cell_type":"code","source":["def image_preprocessing(image):\n","  \"\"\"image preprocessing\n","  transform image pixel value: int [0, 255] -> float [-1.0, 1.0]\n","\n","  Args:\n","    image: PIL image format\n","  \n","  Returns:\n","    image: float type numpy array with shape [1, image_h, image_w, 3] which is in [-1, 1]\n","  \"\"\"\n","  image = np.asarray(image) / 255.\n","  image -= 0.5\n","  image *= 2.0\n","\n","  image = np.expand_dims(image, axis=0)\n","  return image "],"execution_count":0,"outputs":[]},{"metadata":{"id":"fQ_BwrwfNyRt","colab_type":"code","colab":{}},"cell_type":"code","source":["def print_image(image):\n","  \"\"\"print image\n","  \n","  Args:\n","    image: 4-rank np.array [1, h, w, 3]\n","  \"\"\"\n","  print_image = np.squeeze(image, axis=0)\n","  print_image = np.clip(print_image, -1.0, 1.0)\n","  print_image += 1.0\n","  print_image *= 0.5\n","  \n","  plt.axis('off')\n","  plt.imshow(print_image)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GRQiNWVaNyR4","colab_type":"code","colab":{}},"cell_type":"code","source":["content_image_ = image_preprocessing(content_image_)\n","style_image_ = image_preprocessing(style_image_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ixaOkuN-NyR9","colab_type":"code","colab":{}},"cell_type":"code","source":["v = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='vgg_16')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yB4uYH6JNySB","colab_type":"code","colab":{}},"cell_type":"code","source":["v"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mLW1vswdNySK","colab_type":"code","colab":{}},"cell_type":"code","source":["checkpoint_path =  os.path.join(\"./drive/tensorflow_flip/input_data/checkpoints/vgg_16.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nIhYbV2mNySO","colab_type":"code","colab":{}},"cell_type":"code","source":["saver = tf.train.Saver(var_list=v)\n","\n","with tf.Session(config=sess_config) as sess:\n","  sess.run(tf.global_variables_initializer())\n","  # content_image_와 style_image_를 tf.placeholder에 넣고 tf.Variable로 assign\n","  sess.run([content_image_op, style_image_op, generated_image_op],\n","           feed_dict={content_image_p: content_image_,\n","                      style_image_p: style_image_})\n","\n","  _, generated_image_ = sess.run([generated_image_clipping, generated_image])\n","  print_image(content_image_)\n","  print_image(style_image_)\n","  print_image(generated_image_) # initial_image = content_image + small noise\n","\n","  # use saver object to load variables from the saved model\n","  saver.restore(sess, checkpoint_path)\n","  \n","  start_time = time.time()\n","  for step in range(max_steps+1):\n","    _, loss_, _, generated_image_ = \\\n","        sess.run([train_op, total_loss, generated_image_clipping, generated_image])\n","    if step % print_steps == 0:\n","      duration = time.time() - start_time\n","      start_time = time.time()\n","      print(\"step: {}  loss: {}  duration: {}\".format(step,\n","                                                      loss_,\n","                                                      duration))\n","      print_image(generated_image_)\n","  print('training done!')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_7OhDK3NNySW","colab_type":"code","colab":{}},"cell_type":"code","source":["def save_image(image, content_image_name, style_image_name):\n","  \"\"\"print image\n","  \n","  Args:\n","    image: 4-rank np.array [1, h, w, 3]\n","    filename: name of saved image\n","  \"\"\"\n","  save_image = np.squeeze(image, axis=0)\n","  save_image = np.clip(save_image, -1.0, 1.0)\n","  save_image += 1.0\n","  save_image /= 2.0\n","  \n","  save_image = Image.fromarray(np.uint8(save_image*255))\n","  filename = os.path.splitext(os.path.basename(content_image_name))[0] + '_'\n","  filename += os.path.splitext(os.path.basename(style_image_name))[0] + '.jpg'\n","  save_image.save(\"./drive/tensorflow_flip/input_data/checkpoints/\" + filename)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oe4ZH6o_N4CW","colab_type":"code","colab":{}},"cell_type":"code","source":["save_image(generated_image_, content_image_name, style_image_name)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RXJJOXsElidt","colab_type":"code","colab":{}},"cell_type":"code","source":["filename = os.path.splitext(os.path.basename(content_image_name))[0] + '_'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JqFW1spElxyz","colab_type":"code","colab":{}},"cell_type":"code","source":["os.path.splitext(os.path.basename(style_image_name))[0]"],"execution_count":0,"outputs":[]}]}