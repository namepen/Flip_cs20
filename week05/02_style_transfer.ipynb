{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer using VGG16 network\n",
    "\n",
    "* `A Neural Algorithm of Artistic Style`, [arXiv:1508.06576](https://arxiv.org/abs/1508.06576)\n",
    "  * Leon A. Gatys, Alexander S. Ecker, and, Matthias Bethge\n",
    "\n",
    "\n",
    "* `models/research/slim/nets`을 이용하여 만듦\n",
    "* content_input_image 최대 길이 max_L=1024 로 고정\n",
    "* style_input_image 최대 길이 max_L=1024 로 고정\n",
    "* 원래 논문처럼 `average_pooling` 사용\n",
    "* loss는 논문에 나온 그대로 사용하지 않음\n",
    "  * Johnson et., al. Perceptual Losses for Real-Time Style Transfer and Super-Resolution 에 나온 loss 참고\n",
    "  * content loss, style loss 둘다 feature map 차원에 맞게 normalize 함\n",
    "* hyperparameter들은 [cs20](http://web.stanford.edu/class/cs20si/) 코드를 참조\n",
    "* input_image는 우리집 고양이\n",
    "* style_image\n",
    "  * [Starry Night](https://en.wikipedia.org/wiki/The_Starry_Night) [Gogh 작품]\n",
    "  * [The Scream](https://en.wikipedia.org/wiki/The_Scream) [Munch 작품]\n",
    "  \n",
    "* reference code [ilguyi's code](https://github.com/ilguyi/style-transfer.tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"$HOME/models/research/slim/\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_dir = '../input_data/'\n",
    "content_image_name = 'my_cat2.jpg'\n",
    "style_image_name = 'Gogh_The_Starry_Night.jpg'\n",
    "noise_ratio = 0.6\n",
    "content_image_max_L = 200 # upper bound of content image size\n",
    "style_image_max_L = 200 # upper bound of style image size\n",
    "style_loss_weight = np.array([0.5, 1.0, 1.5, 3.0, 4.0])\n",
    "style_loss_weight /= np.sum(style_loss_weight)\n",
    "content_weight = 1.0\n",
    "style_weight = 1.0\n",
    "learning_rate = 2.0\n",
    "max_steps = 300\n",
    "print_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a VGG16 graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_16(inputs,\n",
    "           reuse=False,\n",
    "           scope='vgg_16'):\n",
    "  \"\"\"Oxford Net VGG 16-Layers version D Example\n",
    "  \n",
    "  My Note: This code is modified version of vgg_16 which is loacted on `models/research/slim/nets/vgg.py`\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    reuse: whether or not the model is being reused.\n",
    "    scope: Optional scope for the variables.\n",
    "\n",
    "  Returns:\n",
    "    net: the output of the logits layer (if num_classes is a non-zero integer),\n",
    "      or the input to the logits layer (if num_classes is 0 or None).\n",
    "    end_points: a dict of tensors with intermediate activations.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_16', [inputs], reuse=reuse) as sc:\n",
    "    end_points_collection = sc.original_name_scope + '_end_points'\n",
    "    with slim.arg_scope([slim.conv2d, slim.avg_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      # 여기를 직접 채워 넣으시면 됩니다.\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.avg_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.avg_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.avg_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.avg_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.avg_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "\n",
    "      return net, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read content image and style image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_ = Image.open(os.path.join(input_data_dir, content_image_name))\n",
    "style_image_ = Image.open(os.path.join(input_data_dir, style_image_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_resize_with_upper_bound(image, max_L):\n",
    "  \"\"\"Resize images\n",
    "  \n",
    "  Args:\n",
    "    image: PIL image format\n",
    "    max_L: upper bound of the image size\n",
    "    \n",
    "  Returns:\n",
    "    image: resized image with PIL format\n",
    "    h: resized height\n",
    "    w: resized width\n",
    "  \"\"\"\n",
    "  w, h = image.size\n",
    "  if np.max(np.array([h, w])) > max_L:\n",
    "    if h < w:\n",
    "      h = int(max_L * h / w)\n",
    "      w = max_L\n",
    "    else:\n",
    "      w = int(max_L * w / h)\n",
    "      h = max_L\n",
    "  image = image.resize((w, h))\n",
    "  \n",
    "  return image, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_, content_image_h, content_image_w = image_resize_with_upper_bound(content_image_, content_image_max_L)\n",
    "style_image_, style_image_h, style_image_w = image_resize_with_upper_bound(style_image_, style_image_max_L)\n",
    "print('content_image size: height: {}  width: {}'.format(content_image_h, content_image_w))\n",
    "print('style_image size: height: {}  width: {}'.format(style_image_h, style_image_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "content_image_p = tf.placeholder(tf.float32, [1, content_image_h, content_image_w, 3])\n",
    "style_image_p = tf.placeholder(tf.float32, [1, style_image_h, style_image_w, 3])\n",
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "# content_image, style_image를 tf.Variable로 바꾸기 위해 tf.placeholder와 같은 shape의 zero Tensor를 만듦\n",
    "content_image = tf.get_variable(name='content_image',\n",
    "                                shape=[1, content_image_h, content_image_w, 3],\n",
    "                                initializer=tf.zeros_initializer())\n",
    "style_image = tf.get_variable(name='style_image',\n",
    "                              shape=[1, style_image_h, style_image_w, 3],\n",
    "                              initializer=tf.zeros_initializer())\n",
    "generated_image = tf.get_variable(name='generated_image',\n",
    "                                  shape=[1, content_image_h, content_image_w, 3],\n",
    "                                  initializer=tf.random_uniform_initializer(minval=-20, maxval=20))\n",
    "# tf.placeholder를 tf.Variable로 바꿈\n",
    "content_image_op = content_image.assign(content_image_p)\n",
    "style_image_op = style_image.assign(style_image_p)\n",
    "# 초기 이미지는 content_image에 random noise를 섞음\n",
    "generated_image_op = generated_image.assign(generated_image * noise_ratio + \\\n",
    "                                            content_image_p * (1.0 - noise_ratio))\n",
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "# generated_image는 매 update 후에 아래의 값 사이로 clipping\n",
    "norm_means = np.array([123.68, 116.779, 103.939])\n",
    "min_vals = -norm_means\n",
    "max_vals = 255. - norm_means\n",
    "generated_image_clipping = generated_image.assign(tf.clip_by_value(generated_image,\n",
    "                                                                   clip_value_min=min_vals,\n",
    "                                                                   clip_value_max=max_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "_, feature_maps_c = vgg_16(content_image) # input: content_image\n",
    "_, feature_maps_s = vgg_16(style_image, reuse=True) # input: style_image\n",
    "_, feature_maps_g = vgg_16(generated_image, reuse=True) # input: generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  writer = tf.summary.FileWriter(\"./graphs/02_style_transfer\", sess.graph)\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collecte feature maps\n",
    "\n",
    "* content layers\n",
    "  * `conv4_2`: key name -> 'vgg16/vgg_16/conv4/conv4_2'\n",
    "* style layers\n",
    "  * `conv1_1`: key name -> 'vgg16/vgg_16/conv1/conv1_1'\n",
    "  * `conv2_1`: key name -> 'vgg16/vgg_16/conv2/conv2_1'\n",
    "  * `conv3_1`: key name -> 'vgg16/vgg_16/conv3/conv3_1'\n",
    "  * `conv4_1`: key name -> 'vgg16/vgg_16/conv4/conv4_1'\n",
    "  * `conv5_1`: key name -> 'vgg16/vgg_16/conv5/conv5_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = feature_maps_c['vgg_16/conv4/conv4_2']\n",
    "style_layers = [feature_maps_s['vgg_16/conv1/conv1_1'],\n",
    "                feature_maps_s['vgg_16/conv2/conv2_1'],\n",
    "                feature_maps_s['vgg_16/conv3/conv3_1'],\n",
    "                feature_maps_s['vgg_16/conv4/conv4_1'],\n",
    "                feature_maps_s['vgg_16/conv5/conv5_1']]\n",
    "generated_layers = [feature_maps_g['vgg_16/conv4/conv4_2'],\n",
    "                    feature_maps_g['vgg_16/conv1/conv1_1'],\n",
    "                    feature_maps_g['vgg_16/conv2/conv2_1'],\n",
    "                    feature_maps_g['vgg_16/conv3/conv3_1'],\n",
    "                    feature_maps_g['vgg_16/conv4/conv4_1'],\n",
    "                    feature_maps_g['vgg_16/conv5/conv5_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(P, F, scope):\n",
    "  \"\"\"Calculate the content loss function between\n",
    "  the feature maps of content image and generated image.\n",
    "  \n",
    "  Args:\n",
    "    P: the feature maps of the content image\n",
    "    F: the feature maps of the generated image\n",
    "    scope: scope\n",
    "    \n",
    "  Returns:\n",
    "    loss: content loss (mean squared loss)\n",
    "  \"\"\"\n",
    "  # 여기를 직접 채워 넣으시면 됩니다.\n",
    "  with tf.variable_scope(scope):\n",
    "    assert F.shape == P.shape\n",
    "    loss = tf.losses.mean_squared_error(F, P)\n",
    "    #loss = 0.5 * tf.reduce_sum(tf.square(F - P)) # original loss on paper\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style_layers, generated_layers, scope):\n",
    "  \"\"\"Calculate the style loss function between\n",
    "  the gram matrix of feature maps of style image and generated image.\n",
    "  \n",
    "  Args:\n",
    "    style_layers: list of the feature maps of the style image\n",
    "    generated_layers: list of the feature maps of the generated image\n",
    "    scope: scope\n",
    "    \n",
    "  Returns:\n",
    "    loss: style loss (mean squared loss)\n",
    "  \"\"\"\n",
    "  def _style_loss_one_layer(feature_map_s, feature_map_g):\n",
    "    \"\"\"Calculate the style loss for one layer.\n",
    "    \n",
    "    Args:\n",
    "      feature_map_s: the feature map of the style image\n",
    "        - G: the gram matrix of the feature_map_s\n",
    "      feature_map_g: the feature map of the generated image\n",
    "        - A: the gram matrix of the feature_map_g\n",
    "      \n",
    "    Returns:\n",
    "      loss: style loss for one layer (mean squared loss)\n",
    "    \"\"\"\n",
    "    #_, h, w, c = feature_map_s.get_shape().as_list()\n",
    "    G = _gram_matrix(feature_map_s)\n",
    "    A = _gram_matrix(feature_map_g)\n",
    "    # 여기를 직접 채워 넣으시면 됩니다.\n",
    "    loss = tf.losses.mean_squared_error(G, A)\n",
    "    return loss\n",
    "  \n",
    "  def _gram_matrix(feature_map):\n",
    "    \"\"\"Calculate the gram matrix for the feature map\n",
    "    \n",
    "    Args:\n",
    "      feature_map: 4-rank Tensor [1, height, width, channels]\n",
    "        - F = 2-rank Tensor [h * w, channels]\n",
    "      \n",
    "    Returns:\n",
    "      gram_matrix: 2-rank Tensor [c, c] (F.transpose x F)\n",
    "    \"\"\"\n",
    "    # 여기를 직접 채워 넣으시면 됩니다.\n",
    "    F = tf.squeeze(feature_map, axis=0)\n",
    "    h, w, c = F.get_shape().as_list()\n",
    "    F = tf.reshape(F, [h * w, c])\n",
    "    # normalize for calculating squared Frobenius norm\n",
    "    gram_matrix = tf.matmul(tf.transpose(F), F) / (h * w)\n",
    "    return gram_matrix\n",
    "    \n",
    "  with tf.variable_scope(scope):\n",
    "    assert len(style_layers) == len(generated_layers)\n",
    "\n",
    "    loss = 0.0\n",
    "    for i in range(len(style_layers)):\n",
    "      loss_one = _style_loss_one_layer(style_layers[i], generated_layers[i])\n",
    "      loss += loss_one * style_loss_weight[i]\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_c = content_loss(content_layers, generated_layers[0],\n",
    "                      scope='content_loss')\n",
    "loss_s = style_loss(style_layers, generated_layers[1:],\n",
    "                    scope='style_loss')\n",
    "\n",
    "with tf.variable_scope('total_loss'):\n",
    "  total_loss = content_weight * loss_c + style_weight * loss_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = opt.minimize(total_loss, var_list=generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore VGG16 weights using `tf.saver.restore`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the VGG16 checkpoint: \n",
    "\n",
    "```\n",
    "$ CHECKPOINT_DIR='./checkpoints'\n",
    "$ mkdir ${CHECKPOINT_DIR}\n",
    "$ cd ${CHECKPOINT_DIR}\n",
    "$ wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n",
    "$ tar -xvf vgg_16_2016_08_28.tar.gz\n",
    "$ rm vgg_16_2016_08_28.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_preprocessing(image):\n",
    "  \"\"\"vgg image preprocessing\n",
    "  output image is applied by mean_image_subtraction\n",
    "  \n",
    "  _R_MEAN = 123.68\n",
    "  _G_MEAN = 116.779\n",
    "  _B_MEAN = 103.939\n",
    "\n",
    "  Args:\n",
    "    image (PIL image): image with shape [height, width, channels]\n",
    "    \n",
    "  Returns:\n",
    "    image (np.int32): np.array with shape [1, 224, 224, 3] applied by mean_image_subtraction\n",
    "  \"\"\"\n",
    "  image = np.asarray(image)\n",
    "  image = image.astype(np.float32)\n",
    "  image[:,:,0] -= 123.68 # for _R_MEAN\n",
    "  image[:,:,1] -= 116.779 # for _G_MEAN\n",
    "  image[:,:,2] -= 103.939 # for _B_MEAN\n",
    "  image = np.expand_dims(image, axis=0)\n",
    "  \n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_image(image):\n",
    "  \"\"\"print image\n",
    "  \n",
    "  Args:\n",
    "    image: 4-rank np.array [1, h, w, 3]\n",
    "  \"\"\"\n",
    "  print_image = np.squeeze(image, axis=0)\n",
    "  print_image[:, :, 0] += 123.68\n",
    "  print_image[:, :, 1] += 116.779\n",
    "  print_image[:, :, 2] += 103.939\n",
    "  print_image = np.clip(print_image, 0, 255).astype('uint8')\n",
    "\n",
    "  plt.axis('off')\n",
    "  plt.imshow(print_image)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_ = vgg_preprocessing(content_image_)\n",
    "style_image_ = vgg_preprocessing(style_image_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='vgg_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(var_list=v)\n",
    "\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  # content_image_와 style_image_를 tf.placeholder에 넣고 tf.Variable로 assign\n",
    "  sess.run([content_image_op, style_image_op, generated_image_op],\n",
    "           feed_dict={content_image_p: content_image_,\n",
    "                      style_image_p: style_image_})\n",
    "\n",
    "  _, generated_image_ = sess.run([generated_image_clipping, generated_image])\n",
    "  print_image(content_image_)\n",
    "  print_image(style_image_)\n",
    "  print_image(generated_image_) # initial_image = content_image + small noise\n",
    "\n",
    "  # use saver object to load variables from the saved model\n",
    "  saver.restore(sess, \"../checkpoints/vgg_16.ckpt\")\n",
    "  \n",
    "  start_time = time.time()\n",
    "  for step in range(max_steps+1):\n",
    "    _, loss_, loss_c_, loss_s_, _, generated_image_ = \\\n",
    "        sess.run([train_op, total_loss, loss_c, loss_s, generated_image_clipping, generated_image])\n",
    "    if (step+1) % print_steps == 0:\n",
    "      duration = time.time() - start_time\n",
    "      start_time = time.time()\n",
    "      print(\"step: {}  total_loss: {}  loss_c: {}  loss_s: {}  duration: {}\".format((step+1), loss_, loss_c_, loss_s_, duration))\n",
    "      print_image(generated_image_)\n",
    "  print('training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, content_image_name, style_image_name):\n",
    "  \"\"\"print image\n",
    "  \n",
    "  Args:\n",
    "    image: 4-rank np.array [1, h, w, 3]\n",
    "    content_image_name: (string) filename of content image\n",
    "    style_image_name: (string) filename of style image\n",
    "  \"\"\"\n",
    "  save_image = np.squeeze(image, axis=0)\n",
    "  save_image[:, :, 0] += 123.68\n",
    "  save_image[:, :, 1] += 116.779\n",
    "  save_image[:, :, 2] += 103.939\n",
    "  save_image = np.clip(save_image, 0, 255)\n",
    "\n",
    "  save_image = Image.fromarray(np.uint8(save_image))\n",
    "  filename = os.path.splitext(os.path.basename(content_image_name))[0] + '_'\n",
    "  filename += os.path.splitext(os.path.basename(style_image_name))[0] + '.jpg'\n",
    "  save_image.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(generated_image_, content_image_name, style_image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
