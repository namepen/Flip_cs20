{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with mnist dataset\n",
    "\n",
    "example의 log_starter 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Starter code for simple logistic regression model for MNIST\n",
    "with tf.data module\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "Created by Chip Huyen (chiphuyen@cs.stanford.edu)\n",
    "CS20: \"TensorFlow for Deep Learning Research\"\n",
    "cs20.stanford.edu\n",
    "Lecture 03\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyper-paramaters\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "n_train = 60000\n",
    "n_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading mnist dataset using keras\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(y_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_train = x_train.reshape([-1,784]) #flatten\n",
    "y_train = np.asarray(y_train, dtype=np.int32)\n",
    "\n",
    "x_test = x_test / 255\n",
    "x_test = x_test.reshape([-1, 784]) #flatten\n",
    "y_test = np.asarray(y_test, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19edbe909e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1000].reshape([28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create datasets and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((?, 784), (?,)), types: (tf.float64, tf.int32)>\n",
      "<BatchDataset shapes: ((?, 784), (?,)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# create training Dataset and batch it\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(10000)\n",
    "train_data = train_data.batch(batch_size)\n",
    "print(train_data)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.batch(batch_size)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one iterator and initialize it with different datasets\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types,\n",
    "                                          train_data.output_shapes)\n",
    "img, label = iterator.get_next()\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "test_init = iterator.make_initializer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.cast(img ,dtype=tf.float32)\n",
    "label = tf.cast(label, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: create weights and bias\n",
    "\n",
    "* w is initialized to random variables with mean of 0, stddev of 0.01 \n",
    "* b is initialized to 0\n",
    "* shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "* shape of b depends on Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.get_variable(name='weigth', shape=(784, 10), initializer=tf.random_normal_initializer(0, 0.01))\n",
    "b = tf.get_variable(name='bias', shape=(1, 10), initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: build model\n",
    "* the model that returns the logits.\n",
    "* this logits will be later passed through softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = tf.matmul(img, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: define loss function\n",
    "* use cross entropy of softmax of logits as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original\n",
    "#entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=label, name='entropy')\n",
    "#loss = tf.reduce_mean(entropy, name='loss')\n",
    "\n",
    "#Other\n",
    "y_one_hot = tf.one_hot(label, depth=10)\n",
    "cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_one_hot, logits=logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: define training op\n",
    "* using gradient descent with learning rate of 0.01 to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: calculate accuracy with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.nn.softmax(logit)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.cast(label, tf.int64))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer\n",
    "writer = tf.summary.FileWriter(logdir='./graphs/log_str/', graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 2.2556700370967517\n",
      "Average loss epoch 1: 2.1656486535631516\n",
      "Average loss epoch 2: 2.0751462783386456\n",
      "Average loss epoch 3: 1.9861966335951393\n",
      "Average loss epoch 4: 1.9002810304861333\n",
      "Average loss epoch 5: 1.8178170314475672\n",
      "Average loss epoch 6: 1.7389544647639747\n",
      "Average loss epoch 7: 1.6644297184974655\n",
      "Average loss epoch 8: 1.5942724762694922\n",
      "Average loss epoch 9: 1.5283081968710113\n",
      "Average loss epoch 10: 1.4663861562956626\n",
      "Average loss epoch 11: 1.4087777043710639\n",
      "Average loss epoch 12: 1.3550552638100664\n",
      "Average loss epoch 13: 1.3048659003873877\n",
      "Average loss epoch 14: 1.258231823632458\n",
      "Average loss epoch 15: 1.2148125082699222\n",
      "Average loss epoch 16: 1.174424140946443\n",
      "Average loss epoch 17: 1.136829639548686\n",
      "Average loss epoch 18: 1.102000654887543\n",
      "Average loss epoch 19: 1.069671294455335\n",
      "Average loss epoch 20: 1.039398170356303\n",
      "Average loss epoch 21: 1.0111564228783791\n",
      "Average loss epoch 22: 0.9848467726697291\n",
      "Average loss epoch 23: 0.9602668770849069\n",
      "Average loss epoch 24: 0.9371758296545635\n",
      "Average loss epoch 25: 0.9155552978200445\n",
      "Average loss epoch 26: 0.8952318471886201\n",
      "Average loss epoch 27: 0.8762015902411455\n",
      "Average loss epoch 28: 0.8582605201044062\n",
      "Average loss epoch 29: 0.8413306196361209\n",
      "Total time: 59.41689109802246 seconds\n",
      "Accuracy 0.8516\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # train the model n_epochs times\n",
    "    for i in range(n_epochs):\n",
    "        sess.run(train_init)\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                _, l = sess.run([optimizer, cross_entropy])\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    sess.run(test_init)\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch = sess.run(accuracy)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "        \n",
    "    print('Accuracy {0}'.format(total_correct_preds/n_test))\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
