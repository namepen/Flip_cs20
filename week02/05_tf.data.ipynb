{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tf.data` API\n",
    "\n",
    "`tf.data` API has two new abstractions\n",
    "* `tf.data.Dataset` represents a sequence of elements, in which each element contains one or more Tensor objects. For example, in an image pipeline, the image data and a label\n",
    "  * Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more `tf.Tensor` objects.\n",
    "    * `tf.data.Dataset.from_tensors()`\n",
    "    * `tf.data.Dataset.from_tensor_slices()`\n",
    "    * `tf.data.TFRecordDataset`:  TFRecord format을 읽을 때\n",
    "  * Applying a transformation (e.g. Dataset.batch()) constructs a dataset from one or more `tf.data.Dataset` objects.\n",
    "* `tf.data.Iterator` provides the main way to extract elements from a dataset. The operation returned by `Iterator.get_next()` yields the next element of a `Dataset` when executed, and typically acts as the interface between input pipeline code and your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset from `tf.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set N=50 for small dataset loading\n",
    "N = 50\n",
    "train_data = train_data[:N]\n",
    "train_labels = train_labels[:N]\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)\n",
    "\n",
    "test_data = test_data[:N]\n",
    "test_labels = test_labels[:N]\n",
    "test_data = test_data / 255.\n",
    "test_labels = np.asarray(test_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "\n",
    "1. You must define a source. `tf.data.Dataset`.\n",
    "  * To construct a Dataset from some tensors in memory, you can use `tf.data.Dataset.from_tensors()` or `tf.data.Dataset.from_tensor_slices()`.\n",
    "  * Other methods\n",
    "    * `tf.data.TextLineDataset(filenames)`\n",
    "    * `tf.data.FixedLengthRecordDataset(filenames)`\n",
    "    * `tf.data.TFRecordDataset(filenames)`\n",
    "2. Transformation\n",
    "  * `Dataset.map()`: to apply a function to each element\n",
    "  * `Dataset.batch()`\n",
    "  * `Dataset.shuffle()`\n",
    "3. `Iterator`\n",
    "  * `Iterator.initializer`: which enables you to (re)initialize the iterator's state\n",
    "  * `Iterator.get_next()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Store data in `tf.data.Dataset`\n",
    "\n",
    "* `tf.data.Dataset.from_tensor_slices((features, labels))`\n",
    "* `tf.data.Dataset.from_generator(gen, output_types, output_shapes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "print(train_dataset)\n",
    "print(train_dataset.output_shapes)\n",
    "print(train_dataset.output_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transformaion\n",
    "\n",
    "* `apply(transformation_func)`\n",
    "* `batch(batch_size)`\n",
    "* `concatenate(dataset)`\n",
    "* `flat_map(map_func)`\n",
    "* `repeat(count=None)`\n",
    "  * count=max_epochs\n",
    "* `shuffle(buffer_size, seed=None, reshuffle_each_iteration=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=2)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterator\n",
    "\n",
    "#### 3.1 `make_one_shot_iterator()`\n",
    "\n",
    "* Creates an Iterator for enumerating the elements of this dataset.\n",
    "  * Note: The returned iterator will be initialized automatically. A \"one-shot\" iterator does not currently support re-initialization.\n",
    "\n",
    "###### Common pattern\n",
    "```python\n",
    "while True:\n",
    "  try:\n",
    "    sess.run(result)\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "x, y = train_iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `for`문으로 epoch control 시 유의점\n",
    "\n",
    "* 사실상 `dataset.repeat(count=2)` 함수로 max_epochs 조절함\n",
    "* `for`문으로 epoch을 조절하고 싶으면 `dataset.repeat()` 쓰지 않으면 됨\n",
    "  * `while`문을 다 돌면 count 만큼의 epochs이 끝남\n",
    "* `for`문으로 control 하고 싶으면 `for`문 시작할 때마다 `iterator.initializer`를 해야 함\n",
    "  * 다음 예제 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "#sess.run(iterator.initializer) 할 필요 없음\n",
    "\n",
    "step = 0\n",
    "max_epochs = 3\n",
    "for epoch in range(max_epochs):\n",
    "  while True:\n",
    "    try:\n",
    "      x_, y_ = sess.run([x, y])\n",
    "\n",
    "      print(\"step: {}  labels: {}\".format(step, y_))\n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 `make_initializable_iterator()`\n",
    "\n",
    "* Creates an Iterator for enumerating the elements of this dataset.\n",
    "* Should `run` the `iterator.initializer`.\n",
    "\n",
    "사용법\n",
    "```python\n",
    "dataset = ...\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "# ...\n",
    "sess.run(iterator.initializer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "\n",
    "x, y = train_iterator.get_next()\n",
    "x = tf.cast(x, dtype = tf.float32)\n",
    "y = tf.cast(y, dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `for`문으로 epoch control 시 유의점\n",
    "\n",
    "* `iterator.initializer`를 통해서 매 `for`문 마다 dataset을 initial 해야함\n",
    "* `N / batch_size`가 나누어 떨어지지 않으면 맨 마지막 배치는 `N % batch_size` 만큼의 데이터만 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "\n",
    "step = 0\n",
    "max_epochs = 3\n",
    "for epoch in range(max_epochs):\n",
    "  sess.run(train_iterator.initializer)\n",
    "  while True:\n",
    "    try:\n",
    "      x_, y_ = sess.run([x, y])\n",
    "\n",
    "      print(\"step: {}  labels: {}\".format(step, y_))\n",
    "      step += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [From TensorFlow official site](https://www.tensorflow.org/programmers_guide/datasets)\n",
    "\n",
    "* 밑에 예제들은 TF 홈페이지에서 가져옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Initialize an iterator over a dataset with 10 elements.\n",
    "  sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "  for i in range(10):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)\n",
    "\n",
    "  # Initialize the same iterator over a dataset with 100 elements.\n",
    "  sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "  for i in range(100):\n",
    "    value = sess.run(next_element)\n",
    "    print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
