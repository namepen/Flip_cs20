{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with mini-batch\n",
    "\n",
    "* y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다. 한 개의 설명 변수에 기반한 경우에는 단순 선형 회귀, 둘 이상의 설명 변수에 기반한 경우에는 다중 선형 회귀라고 한다. [참고: 위키피디아](https://ko.wikipedia.org/wiki/선형_회귀)\n",
    "\n",
    "$$y_{\\textrm{pred}} = \\boldsymbol{W}^{\\top}\\boldsymbol{x} + b$$\n",
    "\n",
    "* $\\boldsymbol{x} = [x_{1}, x_{2}, \\cdots, x_{d}]$\n",
    "* $\\boldsymbol{W} = [w_{1}, w_{2}, \\cdots, w_{d}]$\n",
    "* Loss function: $\\mathcal{L} = \\sum^{N} (y_{\\textrm{pred}} - y)^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pseudo Code\n",
    "\n",
    "```python\n",
    "for epoch in max_epochs: # 1 epoch: 모든 데이터(N)를 한번 학습 시켰을 때\n",
    "  for step in num_batches: # num_batches = int(data_size / batch_size)\n",
    "    1. sampling mini-batches with batch_size\n",
    "      1-1. data augmentation (필요하면)\n",
    "    2. calculate logits\n",
    "    3. calculate loss using logits and labels\n",
    "    4. calculate the gradient with respect to weights\n",
    "    5. update weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "tf.set_random_seed(219)\n",
    "np.random.seed(219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1. Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = 3\n",
    "_b = -3\n",
    "N = 200\n",
    "data_x = np.random.uniform(low=0, high=5, size=N)\n",
    "data_y = _a * data_x + _b + np.random.normal(0, 2, N)\n",
    "\n",
    "plt.plot(data_x, data_y, 'ro')\n",
    "plt.axhline(0, color='black', lw=1)\n",
    "plt.axvline(0, color='black', lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create placeholders for inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "# x, y는 배치 데이터를 받을 수 있는 placeholder\n",
    "# x: inputs\n",
    "x = tf.placeholder(tf.float32, name='x', shape=[None])\n",
    "\n",
    "# y: labels\n",
    "y = tf.placeholder(tf.int32, name='y', shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "# create Variables using `get_variable`\n",
    "W = tf.get_variable(name='weight', initializer=tf.constant(0.0))\n",
    "b = tf.get_variable(name='bias', shape=[],\n",
    "                    initializer=tf.random_normal_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model: $y = Wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "with tf.variable_scope(name_or_scope='y_pred'):\n",
    "  y_pred = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "# 이번에는 Hubber loss를 직접 구현해서 넣어 봅시다.\n",
    "loss = tf.reduce_mean(tf.square(y - y_pred, name='loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기를 직접 채워 넣으시면 됩니다.\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2. Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "* stochastic gradient descent with mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "  # Initialize all variables\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "  writer = tf.summary.FileWriter('graphs/02_linear_reg_with_batch', sess.graph)\n",
    "  writer.close()\n",
    "  \n",
    "  # train the model\n",
    "  max_epochs = 100\n",
    "  total_losses = []\n",
    "  num_batches_per_epoch = int(N / batch_size)\n",
    "  for epoch in range(max_epochs+1):\n",
    "    total_loss = 0.0\n",
    "    for step in range(num_batches_per_epoch):\n",
    "      # 전체 데이터 N개중에 batch_size 만큼 데이터 random 하게 추출\n",
    "      batch_index = np.random.choice(N, size=batch_size)\n",
    "      x_ = data_x[batch_index]\n",
    "      y_ = data_y[batch_index]\n",
    "      # `sess.run` 부분을 직접 채워넣으시면 됩니다.\n",
    "      _, loss_ = sess.run([train_op, loss],\n",
    "                          feed_dict={x: x_, y: y_})\n",
    "      total_loss += loss_ * batch_size\n",
    "      \n",
    "    total_loss /= float(N)\n",
    "    total_losses.append(total_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "      print('Epoch {}: total_loss: {}'.format(epoch, total_loss))\n",
    "      \n",
    "  print('training done!')\n",
    "\n",
    "  W_, b_ = sess.run([W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the results: W and b\n",
    "\n",
    "* 정답 W = 3, b = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W_, b_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_losses, label='total_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_x, data_y, 'ro', label='Real data')\n",
    "plt.plot(data_x, W_ * data_x + b_, lw=5, label='model')\n",
    "plt.axhline(0, color='black', lw=1)\n",
    "plt.axvline(0, color='black', lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
